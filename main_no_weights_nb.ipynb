{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import LeNet\n",
    "from dataloader import MNISTDataLoader\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting accuracy:  0.5\n",
      "Train Epoch: 0 [0/6093 (0%)]\tLoss: 0.693154\n",
      "Train Epoch: 0 [5000/6093 (82%)]\tLoss: 0.689663\n",
      "Train Epoch: 1 [0/6093 (0%)]\tLoss: 0.692904\n",
      "Train Epoch: 1 [5000/6093 (82%)]\tLoss: 0.695546\n",
      "Train Epoch: 2 [0/6093 (0%)]\tLoss: 0.692646\n",
      "Train Epoch: 2 [5000/6093 (82%)]\tLoss: 0.686598\n",
      "Train Epoch: 3 [0/6093 (0%)]\tLoss: 0.686614\n",
      "Train Epoch: 3 [5000/6093 (82%)]\tLoss: 0.688036\n",
      "Train Epoch: 4 [0/6093 (0%)]\tLoss: 0.685200\n",
      "Train Epoch: 4 [5000/6093 (82%)]\tLoss: 0.689057\n",
      "Train Epoch: 5 [0/6093 (0%)]\tLoss: 0.688957\n",
      "Train Epoch: 5 [5000/6093 (82%)]\tLoss: 0.683219\n",
      "Train Epoch: 6 [0/6093 (0%)]\tLoss: 0.687793\n",
      "Train Epoch: 6 [5000/6093 (82%)]\tLoss: 0.689627\n",
      "Train Epoch: 7 [0/6093 (0%)]\tLoss: 0.688465\n",
      "Train Epoch: 7 [5000/6093 (82%)]\tLoss: 0.690802\n",
      "Train Epoch: 8 [0/6093 (0%)]\tLoss: 0.685075\n",
      "Train Epoch: 8 [5000/6093 (82%)]\tLoss: 0.681275\n",
      "Train Epoch: 9 [0/6093 (0%)]\tLoss: 0.679605\n",
      "Train Epoch: 9 [5000/6093 (82%)]\tLoss: 0.677451\n",
      "Current accuracy:  0.615\n",
      "Train Epoch: 0 [0/6093 (0%)]\tLoss: 0.677096\n",
      "Train Epoch: 0 [5000/6093 (82%)]\tLoss: 0.680125\n",
      "Train Epoch: 1 [0/6093 (0%)]\tLoss: 0.680290\n",
      "Train Epoch: 1 [5000/6093 (82%)]\tLoss: 0.677278\n",
      "Train Epoch: 2 [0/6093 (0%)]\tLoss: 0.677472\n",
      "Train Epoch: 2 [5000/6093 (82%)]\tLoss: 0.673773\n",
      "Train Epoch: 3 [0/6093 (0%)]\tLoss: 0.672870\n",
      "Train Epoch: 3 [5000/6093 (82%)]\tLoss: 0.673115\n",
      "Train Epoch: 4 [0/6093 (0%)]\tLoss: 0.673611\n",
      "Train Epoch: 4 [5000/6093 (82%)]\tLoss: 0.668492\n",
      "Train Epoch: 5 [0/6093 (0%)]\tLoss: 0.666803\n",
      "Train Epoch: 5 [5000/6093 (82%)]\tLoss: 0.660944\n",
      "Train Epoch: 6 [0/6093 (0%)]\tLoss: 0.658859\n",
      "Train Epoch: 6 [5000/6093 (82%)]\tLoss: 0.660757\n",
      "Train Epoch: 7 [0/6093 (0%)]\tLoss: 0.656412\n",
      "Train Epoch: 7 [5000/6093 (82%)]\tLoss: 0.653511\n",
      "Train Epoch: 8 [0/6093 (0%)]\tLoss: 0.657505\n",
      "Train Epoch: 8 [5000/6093 (82%)]\tLoss: 0.645240\n",
      "Train Epoch: 9 [0/6093 (0%)]\tLoss: 0.648828\n",
      "Train Epoch: 9 [5000/6093 (82%)]\tLoss: 0.632406\n",
      "Current accuracy:  0.805\n",
      "Train Epoch: 0 [0/6093 (0%)]\tLoss: 0.628719\n",
      "Train Epoch: 0 [5000/6093 (82%)]\tLoss: 0.627314\n",
      "Train Epoch: 1 [0/6093 (0%)]\tLoss: 0.611000\n",
      "Train Epoch: 1 [5000/6093 (82%)]\tLoss: 0.631769\n",
      "Train Epoch: 2 [0/6093 (0%)]\tLoss: 0.615920\n",
      "Train Epoch: 2 [5000/6093 (82%)]\tLoss: 0.593959\n",
      "Train Epoch: 3 [0/6093 (0%)]\tLoss: 0.587401\n",
      "Train Epoch: 3 [5000/6093 (82%)]\tLoss: 0.614744\n",
      "Train Epoch: 4 [0/6093 (0%)]\tLoss: 0.595675\n",
      "Train Epoch: 4 [5000/6093 (82%)]\tLoss: 0.576501\n",
      "Train Epoch: 5 [0/6093 (0%)]\tLoss: 0.574402\n",
      "Train Epoch: 5 [5000/6093 (82%)]\tLoss: 0.563317\n",
      "Train Epoch: 6 [0/6093 (0%)]\tLoss: 0.545680\n",
      "Train Epoch: 6 [5000/6093 (82%)]\tLoss: 0.575311\n",
      "Train Epoch: 7 [0/6093 (0%)]\tLoss: 0.540060\n",
      "Train Epoch: 7 [5000/6093 (82%)]\tLoss: 0.527571\n",
      "Train Epoch: 8 [0/6093 (0%)]\tLoss: 0.587848\n",
      "Train Epoch: 8 [5000/6093 (82%)]\tLoss: 0.564483\n",
      "Train Epoch: 9 [0/6093 (0%)]\tLoss: 0.535114\n",
      "Train Epoch: 9 [5000/6093 (82%)]\tLoss: 0.585682\n",
      "Current accuracy:  0.815\n",
      "Train Epoch: 0 [0/6093 (0%)]\tLoss: 0.539061\n",
      "Train Epoch: 0 [5000/6093 (82%)]\tLoss: 0.500663\n",
      "Train Epoch: 1 [0/6093 (0%)]\tLoss: 0.531148\n",
      "Train Epoch: 1 [5000/6093 (82%)]\tLoss: 0.573970\n",
      "Train Epoch: 2 [0/6093 (0%)]\tLoss: 0.541482\n",
      "Train Epoch: 2 [5000/6093 (82%)]\tLoss: 0.540199\n",
      "Train Epoch: 3 [0/6093 (0%)]\tLoss: 0.482046\n",
      "Train Epoch: 3 [5000/6093 (82%)]\tLoss: 0.562199\n",
      "Train Epoch: 4 [0/6093 (0%)]\tLoss: 0.541213\n",
      "Train Epoch: 4 [5000/6093 (82%)]\tLoss: 0.550559\n",
      "Train Epoch: 5 [0/6093 (0%)]\tLoss: 0.524547\n",
      "Train Epoch: 5 [5000/6093 (82%)]\tLoss: 0.496999\n",
      "Train Epoch: 6 [0/6093 (0%)]\tLoss: 0.503114\n",
      "Train Epoch: 6 [5000/6093 (82%)]\tLoss: 0.519562\n",
      "Train Epoch: 7 [0/6093 (0%)]\tLoss: 0.473925\n",
      "Train Epoch: 7 [5000/6093 (82%)]\tLoss: 0.527754\n",
      "Train Epoch: 8 [0/6093 (0%)]\tLoss: 0.493799\n",
      "Train Epoch: 8 [5000/6093 (82%)]\tLoss: 0.487322\n",
      "Train Epoch: 9 [0/6093 (0%)]\tLoss: 0.525529\n",
      "Train Epoch: 9 [5000/6093 (82%)]\tLoss: 0.511939\n",
      "Current accuracy:  0.865\n",
      "Train Epoch: 0 [0/6093 (0%)]\tLoss: 0.483221\n",
      "Train Epoch: 0 [5000/6093 (82%)]\tLoss: 0.531208\n",
      "Train Epoch: 1 [0/6093 (0%)]\tLoss: 0.505720\n",
      "Train Epoch: 1 [5000/6093 (82%)]\tLoss: 0.464937\n",
      "Train Epoch: 2 [0/6093 (0%)]\tLoss: 0.506883\n",
      "Train Epoch: 2 [5000/6093 (82%)]\tLoss: 0.479033\n",
      "Train Epoch: 3 [0/6093 (0%)]\tLoss: 0.419688\n",
      "Train Epoch: 3 [5000/6093 (82%)]\tLoss: 0.554042\n",
      "Train Epoch: 4 [0/6093 (0%)]\tLoss: 0.455610\n",
      "Train Epoch: 4 [5000/6093 (82%)]\tLoss: 0.487642\n",
      "Train Epoch: 5 [0/6093 (0%)]\tLoss: 0.489639\n",
      "Train Epoch: 5 [5000/6093 (82%)]\tLoss: 0.566207\n",
      "Train Epoch: 6 [0/6093 (0%)]\tLoss: 0.457849\n",
      "Train Epoch: 6 [5000/6093 (82%)]\tLoss: 0.419228\n",
      "Train Epoch: 7 [0/6093 (0%)]\tLoss: 0.456892\n",
      "Train Epoch: 7 [5000/6093 (82%)]\tLoss: 0.485021\n",
      "Train Epoch: 8 [0/6093 (0%)]\tLoss: 0.398681\n",
      "Train Epoch: 8 [5000/6093 (82%)]\tLoss: 0.448980\n",
      "Train Epoch: 9 [0/6093 (0%)]\tLoss: 0.411041\n",
      "Train Epoch: 9 [5000/6093 (82%)]\tLoss: 0.410272\n",
      "Current accuracy:  0.8325\n",
      "Train Epoch: 0 [0/6093 (0%)]\tLoss: 0.417489\n",
      "Train Epoch: 0 [5000/6093 (82%)]\tLoss: 0.353945\n",
      "Train Epoch: 1 [0/6093 (0%)]\tLoss: 0.452440\n",
      "Train Epoch: 1 [5000/6093 (82%)]\tLoss: 0.442255\n",
      "Train Epoch: 2 [0/6093 (0%)]\tLoss: 0.442768\n",
      "Train Epoch: 2 [5000/6093 (82%)]\tLoss: 0.398799\n",
      "Train Epoch: 3 [0/6093 (0%)]\tLoss: 0.449942\n",
      "Train Epoch: 3 [5000/6093 (82%)]\tLoss: 0.394329\n",
      "Train Epoch: 4 [0/6093 (0%)]\tLoss: 0.411157\n",
      "Train Epoch: 4 [5000/6093 (82%)]\tLoss: 0.411789\n",
      "Train Epoch: 5 [0/6093 (0%)]\tLoss: 0.391422\n",
      "Train Epoch: 5 [5000/6093 (82%)]\tLoss: 0.422706\n",
      "Train Epoch: 6 [0/6093 (0%)]\tLoss: 0.404236\n",
      "Train Epoch: 6 [5000/6093 (82%)]\tLoss: 0.454994\n",
      "Train Epoch: 7 [0/6093 (0%)]\tLoss: 0.473861\n",
      "Train Epoch: 7 [5000/6093 (82%)]\tLoss: 0.382793\n",
      "Train Epoch: 8 [0/6093 (0%)]\tLoss: 0.387549\n",
      "Train Epoch: 8 [5000/6093 (82%)]\tLoss: 0.408839\n",
      "Train Epoch: 9 [0/6093 (0%)]\tLoss: 0.370516\n",
      "Train Epoch: 9 [5000/6093 (82%)]\tLoss: 0.363111\n",
      "Current accuracy:  0.835\n",
      "Train Epoch: 0 [0/6093 (0%)]\tLoss: 0.401061\n",
      "Train Epoch: 0 [5000/6093 (82%)]\tLoss: 0.348829\n",
      "Train Epoch: 1 [0/6093 (0%)]\tLoss: 0.388971\n",
      "Train Epoch: 1 [5000/6093 (82%)]\tLoss: 0.310662\n",
      "Train Epoch: 2 [0/6093 (0%)]\tLoss: 0.331777\n",
      "Train Epoch: 2 [5000/6093 (82%)]\tLoss: 0.392058\n",
      "Train Epoch: 3 [0/6093 (0%)]\tLoss: 0.371255\n",
      "Train Epoch: 3 [5000/6093 (82%)]\tLoss: 0.307882\n",
      "Train Epoch: 4 [0/6093 (0%)]\tLoss: 0.345356\n",
      "Train Epoch: 4 [5000/6093 (82%)]\tLoss: 0.348772\n",
      "Train Epoch: 5 [0/6093 (0%)]\tLoss: 0.304562\n",
      "Train Epoch: 5 [5000/6093 (82%)]\tLoss: 0.303542\n",
      "Train Epoch: 6 [0/6093 (0%)]\tLoss: 0.315001\n",
      "Train Epoch: 6 [5000/6093 (82%)]\tLoss: 0.263670\n",
      "Train Epoch: 7 [0/6093 (0%)]\tLoss: 0.295705\n",
      "Train Epoch: 7 [5000/6093 (82%)]\tLoss: 0.278092\n",
      "Train Epoch: 8 [0/6093 (0%)]\tLoss: 0.313752\n",
      "Train Epoch: 8 [5000/6093 (82%)]\tLoss: 0.238096\n",
      "Train Epoch: 9 [0/6093 (0%)]\tLoss: 0.289123\n",
      "Train Epoch: 9 [5000/6093 (82%)]\tLoss: 0.272963\n",
      "Current accuracy:  0.9275\n",
      "Train Epoch: 0 [0/6093 (0%)]\tLoss: 0.312635\n",
      "Train Epoch: 0 [5000/6093 (82%)]\tLoss: 0.242495\n",
      "Train Epoch: 1 [0/6093 (0%)]\tLoss: 0.281712\n",
      "Train Epoch: 1 [5000/6093 (82%)]\tLoss: 0.225411\n",
      "Train Epoch: 2 [0/6093 (0%)]\tLoss: 0.192860\n",
      "Train Epoch: 2 [5000/6093 (82%)]\tLoss: 0.228274\n",
      "Train Epoch: 3 [0/6093 (0%)]\tLoss: 0.203844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 141\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Roughly 1 min per loop\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lcv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m--> 141\u001b[0m     \u001b[43mour_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# Ending accuracy\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m our_model\u001b[38;5;241m.\u001b[39mtest()\n",
      "Cell \u001b[1;32mIn[5], line 39\u001b[0m, in \u001b[0;36mNoReweighting.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(data)\n\u001b[0;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output, target\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m hyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_interval\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jjuus\\anaconda3\\envs\\ECE50024\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jjuus\\anaconda3\\envs\\ECE50024\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def show_images(dataloader, num_images):\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        for i in range(len(data)):\n",
    "            plt.imshow(data[i].squeeze(), cmap='gray')\n",
    "            plt.title(f\"Label: {target[i]}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            num_images -= 1\n",
    "            if num_images == 0:\n",
    "                return\n",
    "\n",
    "class NoReweighting():\n",
    "    def __init__(self, network, hyperparameters, criterion, optimizer, train_loader, test_loader):\n",
    "        self.network = network\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "    def train(self):\n",
    "        # Train the network\n",
    "        for epoch in range(hyperparameters['n_epochs']):\n",
    "            self.network.train()\n",
    "            for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.network(data)\n",
    "\n",
    "                loss = self.criterion(output, target.float())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if batch_idx % hyperparameters['log_interval'] == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(self.test_loader):\n",
    "                output = self.network(data.to(device)).cpu()\n",
    "                batch_size = data.size(0)\n",
    "                total_samples += batch_size\n",
    "                test_loss += self.criterion(output, target.float()).item()\n",
    "                pred = (torch.sigmoid(output) > 0.5).int()\n",
    "                correct += (pred == target.int()).sum().item()\n",
    "\n",
    "        return correct / total_samples \n",
    "\n",
    "hyperparameters = {\n",
    "    'n_epochs' : 10,\n",
    "    'batch_size_train' : 100,\n",
    "    'batch_size_valid' : 10,\n",
    "    'batch_size_test' : 1000,\n",
    "    'learning_rate' : 1e-3,\n",
    "    'momentum' : 0.5,\n",
    "    'log_interval' : 50\n",
    "}\n",
    "\n",
    "avging_size = 5\n",
    "perc_9_arr = [100, 25, 10, 5, 1, 0.5]\n",
    "\n",
    "df = pd.DataFrame(columns=[str(x) for x in perc_9_arr])\n",
    "\n",
    "# for perc in perc_9_arr:\n",
    "#     acc_arr = []\n",
    "#     for repeat in range(avging_size):\n",
    "network = LeNet()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "criterion_mean = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD(network.params(),\n",
    "                        lr=hyperparameters['learning_rate'],\n",
    "                        momentum=hyperparameters['momentum'])\n",
    "# # Load the data\n",
    "# data_loader = MNISTDataLoader(validation_ratio=0.05,\n",
    "#                             batch_size_train=hyperparameters['batch_size_train'],\n",
    "#                             batch_size_valid=hyperparameters['batch_size_valid'],\n",
    "#                             batch_size_test=hyperparameters['batch_size_test'])\n",
    "\n",
    "# desired_sample_distribution = [100, perc]\n",
    "# data_loader.sample_bias(desired_sample_distribution, dataset=\"train\")\n",
    "\n",
    "# train_loader = data_loader.train_dataloader\n",
    "# valid_loader = data_loader.valid_dataloader\n",
    "# test_loader = data_loader.test_dataloader\n",
    "\n",
    "train_folder = './dataset-ninja/train_binary'\n",
    "test_folder = './dataset-ninja/test_binary'\n",
    "validate_folder = './dataset-ninja/validate_binary'\n",
    "\n",
    "class_weights = [0.5, 0.5]  # Example weights for each class\n",
    "# class_weights = [804/161429, 160625/161429]  # Oversampling weights\n",
    "# Train count:  {'car': 160625, 'motorcycle': 804}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=(1/2.903307641932519,), std=(0.17295126362098218,)),  # Normalize images\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_folder, transform=transform)\n",
    "weights = [class_weights[label] for label in train_dataset.targets]\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_folder, transform=transform)\n",
    "\n",
    "validate_dataset = datasets.ImageFolder(validate_folder, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size_train'], sampler=train_sampler)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparameters['batch_size_test'], shuffle=True)\n",
    "valid_loader = DataLoader(validate_dataset, batch_size=hyperparameters['batch_size_valid'], shuffle=True)\n",
    "\n",
    "# show_images(test_loader, 100)\n",
    "\n",
    "our_model = NoReweighting(network, hyperparameters, criterion_mean, optimizer, train_loader, test_loader)\n",
    "\n",
    "# Starting accuracy\n",
    "accuracy = our_model.test()\n",
    "print(\"Starting accuracy: \", accuracy)\n",
    "\n",
    "# Roughly 1 min per loop\n",
    "for lcv in range(30):\n",
    "    our_model.train()\n",
    "\n",
    "    # Ending accuracy\n",
    "    accuracy = our_model.test()\n",
    "    print(\"Current accuracy: \", accuracy)\n",
    "#         acc_arr.append(accuracy)\n",
    "#         # print(\"testing \" + str(perc) + \" accuracy = \" + str(accuracy))\n",
    "#     df[str(perc)] = acc_arr\n",
    "#     print(df)\n",
    "# print(df)\n",
    "# df.to_csv(\"accuracy_tsting_no_weights.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        ...,\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]])\n",
      "tensor(0.0080)\n",
      "tensor(0.9482)\n"
     ]
    }
   ],
   "source": [
    "# Testing MNIST dataset\n",
    "hyperparameters = {\n",
    "    'n_epochs' : 10,\n",
    "    'batch_size_train' : 100,\n",
    "    'batch_size_valid' : 10,\n",
    "    'batch_size_test' : 1000,\n",
    "    'learning_rate' : 1e-3,\n",
    "    'momentum' : 0.5,\n",
    "    'log_interval' : 50\n",
    "}\n",
    "\n",
    "data_loader = MNISTDataLoader(validation_ratio=0.05,\n",
    "                            batch_size_train=hyperparameters['batch_size_train'],\n",
    "                            batch_size_valid=hyperparameters['batch_size_valid'],\n",
    "                            batch_size_test=hyperparameters['batch_size_test'])\n",
    "\n",
    "desired_sample_distribution = [100, 100]\n",
    "data_loader.sample_bias(desired_sample_distribution, dataset=\"train\")\n",
    "\n",
    "train_loader = data_loader.train_dataloader\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print\n",
    "\n",
    "print(data[0, 0, :, :])\n",
    "\n",
    "print(torch.mean(data[0, 0, :, :]))\n",
    "print(torch.std(torch.flatten(data[0, 0, :, :])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean and std for binary traffic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [00:25, 78.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0483,  0.0422, -0.0936,  ..., -0.9988, -1.0440, -1.2025],\n",
      "        [ 0.3817,  0.4043,  0.2685,  ..., -1.1798, -1.1346, -1.2251],\n",
      "        [-0.1162, -0.0936, -0.1388,  ..., -1.0667, -0.9083, -0.9083],\n",
      "        ...,\n",
      "        [-0.4557, -1.0440, -1.3835,  ..., -1.2930, -1.0214, -0.7951],\n",
      "        [-0.4104, -0.9988, -1.3382,  ..., -1.2703, -1.0214, -0.8177],\n",
      "        [-0.3425, -0.9535, -1.2930,  ..., -1.2251, -0.9762, -0.7951]])\n",
      "-0.00014798760817193233\n",
      "1.0143969238186596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing Car dataset\n",
    "train_folder = './dataset-ninja/train_binary'\n",
    "test_folder = './dataset-ninja/test_binary'\n",
    "validate_folder = './dataset-ninja/validate_binary'\n",
    "\n",
    "# class_weights = [0.5, 0.5]  # Example weights for each class\n",
    "class_weights = [804/161429, 160625/161429]  # Oversampling weights\n",
    "\n",
    "# Bicycle and bus transform\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Grayscale(),\n",
    "#     transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "#     transforms.Normalize(mean=(1/2.803307641932519,), std=(0.16677909497124954,)),  # Normalize images\n",
    "# ])\n",
    "\n",
    "# Stop sign and traffic light transform\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Grayscale(),\n",
    "#     transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "#     transforms.Normalize(mean=(1/2.423307641932519,), std=(0.22497258224031513,)),  # Normalize images\n",
    "# ])\n",
    "\n",
    "# Cars and motorcycles\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=(1/2.893307641932519,), std=(0.17328706989317727,)),  # Normalize images\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_folder, transform=transform)\n",
    "weights = [class_weights[label] for label in train_dataset.targets]\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, sampler=train_sampler)\n",
    "\n",
    "data_vec = np.array([])\n",
    "for batch_idx, (data, target) in tqdm(enumerate(train_loader)):\n",
    "    data_vec = np.append(data_vec, torch.flatten(data[0, 0, :, :]).numpy())\n",
    "    if batch_idx > 2000:\n",
    "        break\n",
    "\n",
    "print(data[0, 0, :, :])\n",
    "\n",
    "print(np.mean(data_vec))\n",
    "print(np.std(data_vec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE50024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
