{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from model import AlexNet, LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Test learning to reweight model\n",
    "# from main import Reweighting\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)\n",
    "\n",
    "class Reweighting():\n",
    "    def __init__(self, network, hyperparameters, criterion, criterion_mean, optimizer, train_loader, valid_loader, test_loader):\n",
    "        self.network = network.requires_grad_(requires_grad=True)\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion_mean = criterion_mean\n",
    "        self.gradient_network = None\n",
    "\n",
    "    def train(self):\n",
    "        # Train the network\n",
    "        for epoch in range(self.hyperparameters['n_epochs']):\n",
    "            self.network.train()\n",
    "            for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.network(data)\n",
    "                # print(output)\n",
    "                loss = self.criterion_mean(output, target)\n",
    "                # print(loss)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if batch_idx % self.hyperparameters['log_interval'] == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(self.train_loader.dataset),\n",
    "                        100. * batch_idx / len(self.train_loader), loss.item()))\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        self.network.eval()\n",
    "\n",
    "        acc = []\n",
    "        for itr,(test_img, test_label) in tqdm(enumerate(self.test_loader)):\n",
    "            prediction = self.network(test_img.to(device)).detach().cpu().numpy()\n",
    "            # print(prediction)\n",
    "            prediction = np.argmax(prediction, axis=1)\n",
    "            tmp = (prediction == test_label.detach().numpy())\n",
    "            # print(prediction)\n",
    "            # print(test_label)\n",
    "            acc.append(tmp)\n",
    "\n",
    "        accuracy = np.concatenate(acc).mean()\n",
    "        return np.round(accuracy*100,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:36,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting accuracy =  1.27\n",
      "Train Epoch: 0 [0/190879 (0%)]\tLoss: 2.083866\n",
      "Train Epoch: 0 [2048/190879 (1%)]\tLoss: 2.083385\n",
      "Train Epoch: 0 [4096/190879 (2%)]\tLoss: 2.081394\n",
      "Train Epoch: 0 [6144/190879 (3%)]\tLoss: 2.078940\n",
      "Train Epoch: 0 [8192/190879 (4%)]\tLoss: 2.076996\n",
      "Train Epoch: 0 [10240/190879 (5%)]\tLoss: 2.075287\n",
      "Train Epoch: 0 [12288/190879 (6%)]\tLoss: 2.072867\n",
      "Train Epoch: 0 [14336/190879 (7%)]\tLoss: 2.070789\n",
      "Train Epoch: 0 [16384/190879 (9%)]\tLoss: 2.068324\n",
      "Train Epoch: 0 [18432/190879 (10%)]\tLoss: 2.067001\n",
      "Train Epoch: 0 [20480/190879 (11%)]\tLoss: 2.064949\n",
      "Train Epoch: 0 [22528/190879 (12%)]\tLoss: 2.062800\n",
      "Train Epoch: 0 [24576/190879 (13%)]\tLoss: 2.060368\n",
      "Train Epoch: 0 [26624/190879 (14%)]\tLoss: 2.058346\n",
      "Train Epoch: 0 [28672/190879 (15%)]\tLoss: 2.056059\n",
      "Train Epoch: 0 [30720/190879 (16%)]\tLoss: 2.054125\n",
      "Train Epoch: 0 [32768/190879 (17%)]\tLoss: 2.051368\n",
      "Train Epoch: 0 [34816/190879 (18%)]\tLoss: 2.048727\n",
      "Train Epoch: 0 [36864/190879 (19%)]\tLoss: 2.047882\n",
      "Train Epoch: 0 [38912/190879 (20%)]\tLoss: 2.045472\n",
      "Train Epoch: 0 [40960/190879 (21%)]\tLoss: 2.043459\n",
      "Train Epoch: 0 [43008/190879 (22%)]\tLoss: 2.038998\n",
      "Train Epoch: 0 [45056/190879 (24%)]\tLoss: 2.037452\n",
      "Train Epoch: 0 [47104/190879 (25%)]\tLoss: 2.035620\n",
      "Train Epoch: 0 [49152/190879 (26%)]\tLoss: 2.036397\n",
      "Train Epoch: 0 [51200/190879 (27%)]\tLoss: 2.032290\n",
      "Train Epoch: 0 [53248/190879 (28%)]\tLoss: 2.029644\n",
      "Train Epoch: 0 [55296/190879 (29%)]\tLoss: 2.026579\n",
      "Train Epoch: 0 [57344/190879 (30%)]\tLoss: 2.025887\n",
      "Train Epoch: 0 [59392/190879 (31%)]\tLoss: 2.020591\n",
      "Train Epoch: 0 [61440/190879 (32%)]\tLoss: 2.020106\n",
      "Train Epoch: 0 [63488/190879 (33%)]\tLoss: 2.015203\n",
      "Train Epoch: 0 [65536/190879 (34%)]\tLoss: 2.013655\n",
      "Train Epoch: 0 [67584/190879 (35%)]\tLoss: 2.013850\n",
      "Train Epoch: 0 [69632/190879 (36%)]\tLoss: 2.008833\n",
      "Train Epoch: 0 [71680/190879 (37%)]\tLoss: 2.007339\n",
      "Train Epoch: 0 [73728/190879 (39%)]\tLoss: 2.005268\n",
      "Train Epoch: 0 [75776/190879 (40%)]\tLoss: 1.999542\n",
      "Train Epoch: 0 [77824/190879 (41%)]\tLoss: 1.998866\n",
      "Train Epoch: 0 [79872/190879 (42%)]\tLoss: 1.994710\n",
      "Train Epoch: 0 [81920/190879 (43%)]\tLoss: 1.989976\n",
      "Train Epoch: 0 [83968/190879 (44%)]\tLoss: 1.983495\n",
      "Train Epoch: 0 [86016/190879 (45%)]\tLoss: 1.983307\n",
      "Train Epoch: 0 [88064/190879 (46%)]\tLoss: 1.983639\n",
      "Train Epoch: 0 [90112/190879 (47%)]\tLoss: 1.974496\n",
      "Train Epoch: 0 [92160/190879 (48%)]\tLoss: 1.972714\n",
      "Train Epoch: 0 [94208/190879 (49%)]\tLoss: 1.966580\n",
      "Train Epoch: 0 [96256/190879 (50%)]\tLoss: 1.959204\n",
      "Train Epoch: 0 [98304/190879 (51%)]\tLoss: 1.955431\n",
      "Train Epoch: 0 [100352/190879 (52%)]\tLoss: 1.954759\n",
      "Train Epoch: 0 [102400/190879 (53%)]\tLoss: 1.946152\n",
      "Train Epoch: 0 [104448/190879 (55%)]\tLoss: 1.942791\n",
      "Train Epoch: 0 [106496/190879 (56%)]\tLoss: 1.932549\n",
      "Train Epoch: 0 [108544/190879 (57%)]\tLoss: 1.926767\n",
      "Train Epoch: 0 [110592/190879 (58%)]\tLoss: 1.923214\n",
      "Train Epoch: 0 [112640/190879 (59%)]\tLoss: 1.907507\n",
      "Train Epoch: 0 [114688/190879 (60%)]\tLoss: 1.901830\n",
      "Train Epoch: 0 [116736/190879 (61%)]\tLoss: 1.888312\n",
      "Train Epoch: 0 [118784/190879 (62%)]\tLoss: 1.888268\n",
      "Train Epoch: 0 [120832/190879 (63%)]\tLoss: 1.871849\n",
      "Train Epoch: 0 [122880/190879 (64%)]\tLoss: 1.857875\n",
      "Train Epoch: 0 [124928/190879 (65%)]\tLoss: 1.851574\n",
      "Train Epoch: 0 [126976/190879 (66%)]\tLoss: 1.810250\n",
      "Train Epoch: 0 [129024/190879 (67%)]\tLoss: 1.796709\n",
      "Train Epoch: 0 [131072/190879 (68%)]\tLoss: 1.761375\n",
      "Train Epoch: 0 [133120/190879 (70%)]\tLoss: 1.746902\n",
      "Train Epoch: 0 [135168/190879 (71%)]\tLoss: 1.701753\n",
      "Train Epoch: 0 [137216/190879 (72%)]\tLoss: 1.690609\n",
      "Train Epoch: 0 [139264/190879 (73%)]\tLoss: 1.637642\n",
      "Train Epoch: 0 [141312/190879 (74%)]\tLoss: 1.590960\n",
      "Train Epoch: 0 [143360/190879 (75%)]\tLoss: 1.516735\n",
      "Train Epoch: 0 [145408/190879 (76%)]\tLoss: 1.453669\n",
      "Train Epoch: 0 [147456/190879 (77%)]\tLoss: 1.435802\n",
      "Train Epoch: 0 [149504/190879 (78%)]\tLoss: 1.384635\n",
      "Train Epoch: 0 [151552/190879 (79%)]\tLoss: 1.297967\n",
      "Train Epoch: 0 [153600/190879 (80%)]\tLoss: 1.207312\n",
      "Train Epoch: 0 [155648/190879 (81%)]\tLoss: 1.278576\n",
      "Train Epoch: 0 [157696/190879 (82%)]\tLoss: 1.224577\n",
      "Train Epoch: 0 [159744/190879 (83%)]\tLoss: 1.275914\n",
      "Train Epoch: 0 [161792/190879 (84%)]\tLoss: 1.262414\n",
      "Train Epoch: 0 [163840/190879 (86%)]\tLoss: 1.227265\n",
      "Train Epoch: 0 [165888/190879 (87%)]\tLoss: 1.172324\n",
      "Train Epoch: 0 [167936/190879 (88%)]\tLoss: 1.230151\n",
      "Train Epoch: 0 [169984/190879 (89%)]\tLoss: 1.235584\n",
      "Train Epoch: 0 [172032/190879 (90%)]\tLoss: 1.194507\n",
      "Train Epoch: 0 [174080/190879 (91%)]\tLoss: 1.239941\n",
      "Train Epoch: 0 [176128/190879 (92%)]\tLoss: 1.184215\n",
      "Train Epoch: 0 [178176/190879 (93%)]\tLoss: 1.144492\n",
      "Train Epoch: 0 [180224/190879 (94%)]\tLoss: 1.177769\n",
      "Train Epoch: 0 [182272/190879 (95%)]\tLoss: 1.127976\n",
      "Train Epoch: 0 [184320/190879 (96%)]\tLoss: 1.148092\n",
      "Train Epoch: 0 [186368/190879 (97%)]\tLoss: 1.183911\n",
      "Train Epoch: 0 [188416/190879 (98%)]\tLoss: 1.206679\n",
      "Train Epoch: 0 [77190/190879 (99%)]\tLoss: 1.193302\n",
      "Train Epoch: 1 [0/190879 (0%)]\tLoss: 1.082259\n",
      "Train Epoch: 1 [2048/190879 (1%)]\tLoss: 1.128366\n",
      "Train Epoch: 1 [4096/190879 (2%)]\tLoss: 1.178302\n",
      "Train Epoch: 1 [6144/190879 (3%)]\tLoss: 1.180902\n",
      "Train Epoch: 1 [8192/190879 (4%)]\tLoss: 1.176010\n",
      "Train Epoch: 1 [10240/190879 (5%)]\tLoss: 1.131486\n",
      "Train Epoch: 1 [12288/190879 (6%)]\tLoss: 1.185487\n",
      "Train Epoch: 1 [14336/190879 (7%)]\tLoss: 1.228394\n",
      "Train Epoch: 1 [16384/190879 (9%)]\tLoss: 1.194690\n",
      "Train Epoch: 1 [18432/190879 (10%)]\tLoss: 1.151778\n",
      "Train Epoch: 1 [20480/190879 (11%)]\tLoss: 1.201909\n",
      "Train Epoch: 1 [22528/190879 (12%)]\tLoss: 1.163389\n",
      "Train Epoch: 1 [24576/190879 (13%)]\tLoss: 1.158414\n",
      "Train Epoch: 1 [26624/190879 (14%)]\tLoss: 1.182197\n",
      "Train Epoch: 1 [28672/190879 (15%)]\tLoss: 1.146222\n",
      "Train Epoch: 1 [30720/190879 (16%)]\tLoss: 1.145249\n",
      "Train Epoch: 1 [32768/190879 (17%)]\tLoss: 1.178615\n",
      "Train Epoch: 1 [34816/190879 (18%)]\tLoss: 1.133873\n",
      "Train Epoch: 1 [36864/190879 (19%)]\tLoss: 1.195061\n",
      "Train Epoch: 1 [38912/190879 (20%)]\tLoss: 1.177044\n",
      "Train Epoch: 1 [40960/190879 (21%)]\tLoss: 1.241807\n",
      "Train Epoch: 1 [43008/190879 (22%)]\tLoss: 1.165977\n",
      "Train Epoch: 1 [45056/190879 (24%)]\tLoss: 1.142818\n",
      "Train Epoch: 1 [47104/190879 (25%)]\tLoss: 1.146117\n",
      "Train Epoch: 1 [49152/190879 (26%)]\tLoss: 1.144665\n",
      "Train Epoch: 1 [51200/190879 (27%)]\tLoss: 1.175840\n",
      "Train Epoch: 1 [53248/190879 (28%)]\tLoss: 1.215214\n",
      "Train Epoch: 1 [55296/190879 (29%)]\tLoss: 1.148500\n",
      "Train Epoch: 1 [57344/190879 (30%)]\tLoss: 1.125126\n",
      "Train Epoch: 1 [59392/190879 (31%)]\tLoss: 1.188850\n",
      "Train Epoch: 1 [61440/190879 (32%)]\tLoss: 1.139369\n",
      "Train Epoch: 1 [63488/190879 (33%)]\tLoss: 1.155078\n",
      "Train Epoch: 1 [65536/190879 (34%)]\tLoss: 1.140084\n",
      "Train Epoch: 1 [67584/190879 (35%)]\tLoss: 1.191792\n",
      "Train Epoch: 1 [69632/190879 (36%)]\tLoss: 1.180953\n",
      "Train Epoch: 1 [71680/190879 (37%)]\tLoss: 1.130990\n",
      "Train Epoch: 1 [73728/190879 (39%)]\tLoss: 1.155298\n",
      "Train Epoch: 1 [75776/190879 (40%)]\tLoss: 1.140482\n",
      "Train Epoch: 1 [77824/190879 (41%)]\tLoss: 1.118699\n",
      "Train Epoch: 1 [79872/190879 (42%)]\tLoss: 1.177472\n",
      "Train Epoch: 1 [81920/190879 (43%)]\tLoss: 1.127263\n",
      "Train Epoch: 1 [83968/190879 (44%)]\tLoss: 1.222531\n",
      "Train Epoch: 1 [86016/190879 (45%)]\tLoss: 1.172134\n",
      "Train Epoch: 1 [88064/190879 (46%)]\tLoss: 1.188784\n",
      "Train Epoch: 1 [90112/190879 (47%)]\tLoss: 1.136155\n",
      "Train Epoch: 1 [92160/190879 (48%)]\tLoss: 1.224828\n",
      "Train Epoch: 1 [94208/190879 (49%)]\tLoss: 1.138964\n",
      "Train Epoch: 1 [96256/190879 (50%)]\tLoss: 1.185670\n",
      "Train Epoch: 1 [98304/190879 (51%)]\tLoss: 1.138425\n",
      "Train Epoch: 1 [100352/190879 (52%)]\tLoss: 1.213195\n",
      "Train Epoch: 1 [102400/190879 (53%)]\tLoss: 1.163541\n",
      "Train Epoch: 1 [104448/190879 (55%)]\tLoss: 1.116437\n",
      "Train Epoch: 1 [106496/190879 (56%)]\tLoss: 1.132138\n",
      "Train Epoch: 1 [108544/190879 (57%)]\tLoss: 1.204671\n",
      "Train Epoch: 1 [110592/190879 (58%)]\tLoss: 1.176466\n",
      "Train Epoch: 1 [112640/190879 (59%)]\tLoss: 1.144054\n",
      "Train Epoch: 1 [114688/190879 (60%)]\tLoss: 1.185014\n",
      "Train Epoch: 1 [116736/190879 (61%)]\tLoss: 1.174049\n",
      "Train Epoch: 1 [118784/190879 (62%)]\tLoss: 1.112451\n",
      "Train Epoch: 1 [120832/190879 (63%)]\tLoss: 1.220156\n",
      "Train Epoch: 1 [122880/190879 (64%)]\tLoss: 1.201734\n",
      "Train Epoch: 1 [124928/190879 (65%)]\tLoss: 1.187910\n",
      "Train Epoch: 1 [126976/190879 (66%)]\tLoss: 1.155536\n",
      "Train Epoch: 1 [129024/190879 (67%)]\tLoss: 1.158680\n",
      "Train Epoch: 1 [131072/190879 (68%)]\tLoss: 1.177358\n",
      "Train Epoch: 1 [133120/190879 (70%)]\tLoss: 1.168881\n",
      "Train Epoch: 1 [135168/190879 (71%)]\tLoss: 1.127586\n",
      "Train Epoch: 1 [137216/190879 (72%)]\tLoss: 1.170514\n",
      "Train Epoch: 1 [139264/190879 (73%)]\tLoss: 1.163993\n",
      "Train Epoch: 1 [141312/190879 (74%)]\tLoss: 1.191791\n",
      "Train Epoch: 1 [143360/190879 (75%)]\tLoss: 1.102245\n",
      "Train Epoch: 1 [145408/190879 (76%)]\tLoss: 1.146799\n",
      "Train Epoch: 1 [147456/190879 (77%)]\tLoss: 1.103913\n",
      "Train Epoch: 1 [149504/190879 (78%)]\tLoss: 1.220891\n",
      "Train Epoch: 1 [151552/190879 (79%)]\tLoss: 1.135770\n",
      "Train Epoch: 1 [153600/190879 (80%)]\tLoss: 1.138554\n",
      "Train Epoch: 1 [155648/190879 (81%)]\tLoss: 1.207705\n",
      "Train Epoch: 1 [157696/190879 (82%)]\tLoss: 1.183224\n",
      "Train Epoch: 1 [159744/190879 (83%)]\tLoss: 1.221521\n",
      "Train Epoch: 1 [161792/190879 (84%)]\tLoss: 1.132797\n",
      "Train Epoch: 1 [163840/190879 (86%)]\tLoss: 1.128009\n",
      "Train Epoch: 1 [165888/190879 (87%)]\tLoss: 1.157457\n",
      "Train Epoch: 1 [167936/190879 (88%)]\tLoss: 1.225160\n",
      "Train Epoch: 1 [169984/190879 (89%)]\tLoss: 1.173488\n",
      "Train Epoch: 1 [172032/190879 (90%)]\tLoss: 1.126554\n",
      "Train Epoch: 1 [174080/190879 (91%)]\tLoss: 1.149728\n",
      "Train Epoch: 1 [176128/190879 (92%)]\tLoss: 1.174015\n",
      "Train Epoch: 1 [178176/190879 (93%)]\tLoss: 1.183177\n",
      "Train Epoch: 1 [180224/190879 (94%)]\tLoss: 1.138523\n",
      "Train Epoch: 1 [182272/190879 (95%)]\tLoss: 1.174875\n",
      "Train Epoch: 1 [184320/190879 (96%)]\tLoss: 1.200887\n",
      "Train Epoch: 1 [186368/190879 (97%)]\tLoss: 1.124054\n",
      "Train Epoch: 1 [188416/190879 (98%)]\tLoss: 1.181985\n",
      "Train Epoch: 1 [77190/190879 (99%)]\tLoss: 1.230924\n",
      "Train Epoch: 2 [0/190879 (0%)]\tLoss: 1.195876\n",
      "Train Epoch: 2 [2048/190879 (1%)]\tLoss: 1.165005\n",
      "Train Epoch: 2 [4096/190879 (2%)]\tLoss: 1.169632\n",
      "Train Epoch: 2 [6144/190879 (3%)]\tLoss: 1.148569\n",
      "Train Epoch: 2 [8192/190879 (4%)]\tLoss: 1.162974\n",
      "Train Epoch: 2 [10240/190879 (5%)]\tLoss: 1.179666\n",
      "Train Epoch: 2 [12288/190879 (6%)]\tLoss: 1.152888\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m start_accuracy \u001b[38;5;241m=\u001b[39m our_model\u001b[38;5;241m.\u001b[39mtest()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting accuracy = \u001b[39m\u001b[38;5;124m\"\u001b[39m, start_accuracy)\n\u001b[1;32m---> 53\u001b[0m \u001b[43mour_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m end_accuracy \u001b[38;5;241m=\u001b[39m our_model\u001b[38;5;241m.\u001b[39mtest()\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnding accuracy = \u001b[39m\u001b[38;5;124m\"\u001b[39m, end_accuracy)\n",
      "Cell \u001b[1;32mIn[2], line 41\u001b[0m, in \u001b[0;36mReweighting.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_interval\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Epoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     40\u001b[0m         epoch, batch_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(data), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader\u001b[38;5;241m.\u001b[39mdataset),\n\u001b[1;32m---> 41\u001b[0m         \u001b[38;5;241m100.\u001b[39m \u001b[38;5;241m*\u001b[39m batch_idx \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader), \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_folder = '../dataset-ninja/train_unbiased'\n",
    "test_folder = '../dataset-ninja/test_unbiased'\n",
    "validate_folder = '../dataset-ninja/validate_unbiased'\n",
    "\n",
    "class_weights = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2]  # Example weights for each class\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[90.2867748375, 88.24014045, 94.9276596], std=[46.843379357493816, 46.59302413225906, 47.86539694070785]),  # Normalize images\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_folder, transform=transform)\n",
    "weights = [class_weights[label] for label in train_dataset.targets]\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_folder, transform=transform)\n",
    "weights = [class_weights[label] for label in test_dataset.targets]\n",
    "test_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "validate_dataset = datasets.ImageFolder(validate_folder, transform=transform)\n",
    "weights = [class_weights[label] for label in validate_dataset.targets]\n",
    "valid_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "\n",
    "# number of epoch and log interval reduced for testing\n",
    "hyperparameters = {\n",
    "    'n_epochs' : 10,\n",
    "    'batch_size' : 1024,\n",
    "    'learning_rate' : 1e-3,\n",
    "    'momentum' : 0.5,\n",
    "    'log_interval' : 2\n",
    "}\n",
    "\n",
    "network = AlexNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "criterion_mean = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD(network.params(),\n",
    "                        lr=hyperparameters['learning_rate'],\n",
    "                        momentum=hyperparameters['momentum'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], sampler=train_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparameters['batch_size'], sampler=test_sampler)\n",
    "valid_loader = DataLoader(validate_dataset, batch_size=hyperparameters['batch_size'], sampler=valid_sampler)\n",
    "\n",
    "our_model = Reweighting(network, hyperparameters, criterion, criterion_mean, optimizer, train_loader, valid_loader, test_loader)\n",
    "\n",
    "start_accuracy = our_model.test()\n",
    "print(\"Starting accuracy = \", start_accuracy)\n",
    "\n",
    "our_model.train()\n",
    "\n",
    "end_accuracy = our_model.test()\n",
    "print(\"Ending accuracy = \", end_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 2, 2, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 7, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:03,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 7,  ..., 2, 2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:05,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([6, 2, 2,  ..., 2, 7, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:06,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 7,  ..., 4, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:07,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:08,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 4,  ..., 2, 7, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:09,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 6, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:10,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 7, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:12,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 2, 2, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:13,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:14,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([4, 2, 2,  ..., 4, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:15,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 2,  ..., 2, 2, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:16,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 7, 7,  ..., 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:18,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 7,  ..., 2, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:19,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 4,  ..., 4, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:20,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([4, 7, 2,  ..., 6, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:21,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 6, 7,  ..., 2, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:22,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 6,  ..., 2, 4, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:24,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 2,  ..., 7, 2, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:25,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 6, 2,  ..., 2, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:26,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:27,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 7,  ..., 6, 2, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:27,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "tensor([2, 2, 2, 6, 7, 2, 7, 6, 2, 2, 2, 2, 7, 1, 7, 2, 4, 2, 2, 2, 7, 7, 2, 7,\n",
      "        2, 7, 7, 4, 2, 7, 2, 7, 2, 2, 2, 4, 7, 7, 2, 6, 6, 7, 2, 2, 7, 2, 2, 7,\n",
      "        6, 2, 2, 2, 7, 7, 7, 7, 7, 1, 6, 2, 2, 2, 2, 4, 2, 7, 2, 7, 7, 7, 2, 2,\n",
      "        2, 2, 1, 2, 2, 2, 2, 7, 2, 2, 6, 2, 2, 2, 2, 2, 7, 2, 7, 2, 2, 7, 2, 2,\n",
      "        2, 2, 2, 7, 2, 2, 2, 7, 4, 7, 2, 7, 2, 7, 7, 2, 2, 4, 2, 2, 2, 6, 7, 2,\n",
      "        2, 2, 2, 2, 7, 0, 2, 2, 2, 1, 2, 7, 2, 2, 2, 7, 2, 2, 7, 2, 7, 2, 2, 2,\n",
      "        7, 6, 4, 7, 7, 2, 2, 2, 4, 2, 4, 0, 7, 6, 2, 2, 7, 2, 7, 6, 2, 2, 2, 2,\n",
      "        2, 6, 7, 2, 7, 2, 2, 2, 2, 7, 2, 7, 7, 4, 7, 2, 7, 7, 2, 2, 7, 7, 2, 1,\n",
      "        1, 7, 4, 7, 4, 4, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2,\n",
      "        7, 4, 7, 2, 7, 6, 2, 2, 7, 2, 2, 7, 2, 3, 1, 2, 7, 2, 2, 7, 4, 2, 4, 2,\n",
      "        7, 2, 2, 2, 7, 4, 7, 2, 4, 7, 2, 2, 2, 2, 2, 7, 4, 2, 7, 2, 7, 7, 2, 4,\n",
      "        2, 7, 2, 2, 2, 2, 7, 2, 2, 7, 7, 2, 2, 2, 2, 4, 0, 2, 3, 2, 2, 2, 2, 2,\n",
      "        2, 7, 2, 2, 4, 2, 1, 7, 4, 7, 2, 2, 2, 6, 2, 7, 7, 7, 2, 2])\n",
      "Ending accuracy =  56.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_model.network.eval()\n",
    "\n",
    "acc = []\n",
    "for itr,(test_img, test_label) in tqdm(enumerate(our_model.test_loader)):\n",
    "    prediction = our_model.network(test_img.to(device)).detach().cpu().numpy()\n",
    "    # print(prediction)\n",
    "    prediction = np.argmax(prediction, axis=1)\n",
    "    tmp = (prediction == test_label.detach().numpy())\n",
    "    print(prediction)\n",
    "    print(test_label)\n",
    "    acc.append(tmp)\n",
    "\n",
    "accuracy = np.concatenate(acc).mean()\n",
    "end_accuracy = np.round(accuracy*100,2)\n",
    "\n",
    "print(\"Ending accuracy = \", end_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda:0\n",
      "Starting accuracy =  9.569999694824219\n",
      "Ending accuracy =  18.65999984741211\n"
     ]
    }
   ],
   "source": [
    "## Test control model\n",
    "# from main import Reweighting\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)\n",
    "\n",
    "class NoReweighting():\n",
    "    def __init__(self, network, hyperparameters, criterion, optimizer, train_loader, test_loader):\n",
    "        self.network = network\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.train_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "    def train(self):\n",
    "        # Train the network\n",
    "        for epoch in range(hyperparameters['n_epochs']):\n",
    "            self.network.train()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.network(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                # if batch_idx % hyperparameters['log_interval'] == 0:\n",
    "                #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                #         100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            self.network.eval()\n",
    "            output = self.network(data.to(device)).cpu()\n",
    "            test_loss += self.criterion(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "        # print('\\nTest set: \\nAvg. loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        #     test_loss, correct, len(test_loader.dataset),\n",
    "        #     100.0 * correct / len(test_loader.dataset)))\n",
    "        \n",
    "        return (100.0 * correct / len(test_loader.dataset)).item()\n",
    "\n",
    "# number of epoch and log interval reduced for testing\n",
    "hyperparameters = {\n",
    "    'n_epochs' : 2,\n",
    "    'batch_size_train' : 100,\n",
    "    'batch_size_valid' : 10,\n",
    "    'batch_size_test' : 1000,\n",
    "    'learning_rate' : 1e-3,\n",
    "    'momentum' : 0.5,\n",
    "    'log_interval' : 1\n",
    "}\n",
    "\n",
    "network = LeNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "criterion_mean = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD(network.params(),\n",
    "                        lr=hyperparameters['learning_rate'],\n",
    "                        momentum=hyperparameters['momentum'])\n",
    "# Load the data\n",
    "data_loader = MNISTDataLoader(validation_ratio=0.05,\n",
    "                            batch_size_train=hyperparameters['batch_size_train'],\n",
    "                            batch_size_valid=hyperparameters['batch_size_valid'],\n",
    "                            batch_size_test=hyperparameters['batch_size_test'])\n",
    "\n",
    "# 10% representation by 9's class in training data\n",
    "desired_sample_distribution = [100, 100, 100, 100, 100, 100, 100, 100, 100, 10]\n",
    "data_loader.sample_bias(desired_sample_distribution, dataset=\"train\")\n",
    "\n",
    "\n",
    "train_loader = data_loader.train_dataloader\n",
    "valid_loader = data_loader.valid_dataloader\n",
    "test_loader = data_loader.test_dataloader\n",
    "\n",
    "\n",
    "our_model = NoReweighting(network, hyperparameters, criterion_mean, optimizer, train_loader, test_loader)\n",
    "\n",
    "start_accuracy = our_model.test()\n",
    "print(\"Starting accuracy = \", start_accuracy)\n",
    "\n",
    "our_model.train()\n",
    "\n",
    "end_accuracy = our_model.test()\n",
    "print(\"Ending accuracy = \", end_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE50024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
