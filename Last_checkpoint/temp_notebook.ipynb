{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from model import AlexNet, LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Test learning to reweight model\n",
    "# from main import Reweighting\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)\n",
    "\n",
    "class Reweighting():\n",
    "    def __init__(self, network, hyperparameters, criterion, criterion_mean, optimizer, train_loader, valid_loader, test_loader):\n",
    "        self.network = network.requires_grad_(requires_grad=True)\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion_mean = criterion_mean\n",
    "        self.gradient_network = None\n",
    "    \n",
    "    def paper_train(self):\n",
    "        # X_g = LeNet5()\n",
    "        # print(\"Starting training...\")\n",
    "        X_g, y_g = next(iter(self.valid_loader))\n",
    "        X_g = X_g.to(device)\n",
    "        y_g = y_g.to(device)\n",
    "\n",
    "        y_f_hat = torch.empty(1)\n",
    "        y_f_hat = y_f_hat.to(device)\n",
    "\n",
    "        theta_tp1 = self.network.state_dict()\n",
    "        for epoch in range(self.hyperparameters['n_epochs']):\n",
    "            self.network.train()\n",
    "\n",
    "            self.gradient_network = AlexNet()\n",
    "            self.gradient_network.load_state_dict(theta_tp1)\n",
    "\n",
    "            # get batch of data from train_loader\n",
    "            X_f, y_f = next(iter(self.train_loader))\n",
    "            X_f = X_f.to(device)\n",
    "            y_f = y_f.to(device)\n",
    "\n",
    "            # Line 4\n",
    "            y_f_hat = self.gradient_network(X_f)\n",
    "            \n",
    "            # Line 5\n",
    "            epsilon = torch.zeros(y_f.size(), requires_grad=True)\n",
    "            epsilon = epsilon.to(device)\n",
    "\n",
    "            Costs = self.criterion(y_f_hat, y_f.float())\n",
    "            l_f = torch.sum(torch.mul(Costs, epsilon))\n",
    "\n",
    "            # Line 6\n",
    "            grad_t = torch.autograd.grad(outputs=l_f, inputs=self.gradient_network.params(), create_graph=True)\n",
    "            \n",
    "\n",
    "            # Line 7: manually update the weights of the validation network\n",
    "            lr = self.hyperparameters['learning_rate']\n",
    "            self.gradient_network.update_params_SGD_step(lr, grad_t)\n",
    "\n",
    "            # Line 8\n",
    "            # Model has theta_hat\n",
    "            y_g_hat = self.gradient_network(X_g)\n",
    "\n",
    "            # Line 9\n",
    "            l_g = self.criterion_mean(y_g_hat, y_g.float())\n",
    "\n",
    "            # Line 10\n",
    "            grad_epsilon = torch.autograd.grad(l_g, epsilon, only_inputs=True)[0]\n",
    "\n",
    "            # Line 11\n",
    "            w_tilde = torch.clamp(-grad_epsilon, min=0)\n",
    "\n",
    "            if torch.sum(w_tilde) != 0:\n",
    "                w = w_tilde / torch.sum(w_tilde)\n",
    "            else:\n",
    "                w = w_tilde\n",
    "\n",
    "            # Line 12\n",
    "            y_f_hat = self.network(X_f)\n",
    "            Costs = self.criterion(y_f_hat, y_f.float())\n",
    "            l_f_hat = torch.sum(torch.mul(Costs, w))\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Line 13\n",
    "            l_f_hat.backward()\n",
    "\n",
    "            # Line 14\n",
    "            self.optimizer.step()\n",
    "            # break\n",
    "\n",
    "            theta_tp1 = self.network.state_dict()\n",
    "\n",
    "            if (epoch % self.hyperparameters['log_interval'] == 0):\n",
    "                curr_accuracy = self.test()\n",
    "                print(\"Epoch \" + str(epoch) + \" accuracy = \", curr_accuracy)\n",
    "\n",
    "    def train(self):\n",
    "        # Train the network\n",
    "        for epoch in range(self.hyperparameters['n_epochs']):\n",
    "            self.network.train()\n",
    "            for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.network(data)\n",
    "                # print(output)\n",
    "                # print(target.float())\n",
    "                loss = self.criterion_mean(output, target.float())\n",
    "                # print(loss)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if batch_idx % self.hyperparameters['log_interval'] == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(self.train_loader.dataset),\n",
    "                        100. * batch_idx / len(self.train_loader), loss.item()))\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        self.network.eval()\n",
    "\n",
    "        acc = np.array([])\n",
    "        for itr,(test_img, test_label) in enumerate(self.test_loader):\n",
    "            prediction = self.network(test_img.to(device)).detach().cpu()\n",
    "            # print(prediction)\n",
    "            prediction = (torch.sigmoid(prediction) > 0.5).int().numpy()\n",
    "            tmp = (prediction == test_label.detach().numpy())\n",
    "            # print(prediction)\n",
    "            # print(test_label)\n",
    "            # print(tmp)\n",
    "            # acc.append(tmp)\n",
    "            acc = np.append(acc,tmp)\n",
    "\n",
    "        # print(acc)\n",
    "        accuracy = np.mean(acc)\n",
    "        return np.round(accuracy*100,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting accuracy =  50.0\n",
      "Train Epoch: 0 [0/162204 (0%)]\tLoss: 0.714378\n",
      "Train Epoch: 0 [50000/162204 (29%)]\tLoss: 0.718146\n"
     ]
    }
   ],
   "source": [
    "# train_folder = '../dataset-ninja/train_unbiased'\n",
    "# test_folder = '../dataset-ninja/test_unbiased'\n",
    "# validate_folder = '../dataset-ninja/validate_unbiased'\n",
    "train_folder = '/home/obasit/50024/last_checkpoint/DL-reweighting/Last_checkpoint/dataset/extracted/train_unbiased'\n",
    "test_folder = '/home/obasit/50024/last_checkpoint/DL-reweighting/Last_checkpoint/dataset/extracted/test_unbiased'\n",
    "validate_folder = '/home/obasit/50024/last_checkpoint/DL-reweighting/Last_checkpoint/dataset/extracted/validate_unbiased'\n",
    "\n",
    "class_weights = [0.987, 0.013]  # Example weights for each class\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[90.2867748375, 88.24014045, 94.9276596], std=[46.843379357493816, 46.59302413225906, 47.86539694070785]),  # Normalize images\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_folder, transform=transform)\n",
    "weights = [class_weights[label] for label in train_dataset.targets]\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_folder, transform=transform)\n",
    "\n",
    "validate_dataset = datasets.ImageFolder(validate_folder, transform=transform)\n",
    "\n",
    "\n",
    "# number of epoch and log interval reduced for testing\n",
    "hyperparameters = {\n",
    "    'n_epochs' : 50,\n",
    "    'batch_size' : 10000,\n",
    "    'learning_rate' : 1e-3,\n",
    "    'momentum' : 0.5,\n",
    "    'log_interval' : 5\n",
    "}\n",
    "\n",
    "network = AlexNet()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "# criterion_mean = nn.CrossEntropyLoss(reduction='mean')\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "criterion_mean = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD(network.params(),\n",
    "                        lr=hyperparameters['learning_rate'],\n",
    "                        momentum=hyperparameters['momentum'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], sampler=train_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=True)\n",
    "valid_loader = DataLoader(validate_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "our_model = Reweighting(network, hyperparameters, criterion, criterion_mean, optimizer, train_loader, valid_loader, test_loader)\n",
    "\n",
    "start_accuracy = our_model.test()\n",
    "print(\"Starting accuracy = \", start_accuracy)\n",
    "\n",
    "our_model.train()\n",
    "# our_model.paper_train()\n",
    "\n",
    "end_accuracy = our_model.test()\n",
    "print(\"Ending accuracy = \", end_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE50024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
