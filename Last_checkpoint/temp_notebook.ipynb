{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "\n",
    "from model import AlexNet, LeNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Test learning to reweight model\n",
    "# from main import Reweighting\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)\n",
    "\n",
    "class Reweighting():\n",
    "    def __init__(self, network, hyperparameters, criterion, criterion_mean, optimizer, train_loader, valid_loader, test_loader):\n",
    "        self.network = network.requires_grad_(requires_grad=True)\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion_mean = criterion_mean\n",
    "        self.gradient_network = None\n",
    "\n",
    "    def train(self):\n",
    "        # Train the network\n",
    "        for epoch in range(self.hyperparameters['n_epochs']):\n",
    "            self.network.train()\n",
    "            for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.network(data)\n",
    "                # print(output)\n",
    "                loss = self.criterion_mean(output, target)\n",
    "                # print(loss)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if batch_idx % self.hyperparameters['log_interval'] == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(self.train_loader.dataset),\n",
    "                        100. * batch_idx / len(self.train_loader), loss.item()))\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        self.network.eval()\n",
    "\n",
    "        acc = []\n",
    "        for itr,(test_img, test_label) in enumerate(self.test_loader):\n",
    "            prediction = self.network(test_img.to(device)).detach().cpu().numpy()\n",
    "            # print(prediction)\n",
    "            prediction = np.argmax(prediction, axis=1)\n",
    "            tmp = (prediction == test_label.detach().numpy())\n",
    "            print(prediction)\n",
    "            print(test_label)\n",
    "            acc.append(tmp)\n",
    "\n",
    "        accuracy = np.concatenate(acc).mean()\n",
    "        return np.round(accuracy*100,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "tensor([0, 3, 3, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 0, 1, 3, 2, 0, 2, 2, 0, 3, 3, 1,\n",
      "        0, 1, 2, 1, 2, 2, 3, 2, 1, 0, 1, 0, 3, 3, 2, 1, 1, 1, 1, 1, 3, 0, 1, 3,\n",
      "        1, 1])\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "tensor([2, 1, 3, 3, 1, 2, 2, 3, 0, 0, 3, 1, 0, 1, 1, 1, 0, 3, 3, 0, 2, 3, 2, 0,\n",
      "        1, 2, 0, 0, 2, 2, 1, 3, 1, 2, 3, 1, 3, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0])\n",
      "[0 0 0 0]\n",
      "tensor([1, 0, 2, 1])\n",
      "Starting accuracy =  21.15\n",
      "Train Epoch: 0 [0/833 (0%)]\tLoss: 1.384422\n",
      "Train Epoch: 0 [200/833 (24%)]\tLoss: 1.382826\n",
      "Train Epoch: 0 [400/833 (47%)]\tLoss: 1.387188\n",
      "Train Epoch: 0 [600/833 (71%)]\tLoss: 1.385034\n",
      "Train Epoch: 0 [528/833 (94%)]\tLoss: 1.385242\n",
      "Train Epoch: 1 [0/833 (0%)]\tLoss: 1.381855\n",
      "Train Epoch: 1 [200/833 (24%)]\tLoss: 1.387210\n",
      "Train Epoch: 1 [400/833 (47%)]\tLoss: 1.381660\n",
      "Train Epoch: 1 [600/833 (71%)]\tLoss: 1.382896\n",
      "Train Epoch: 1 [528/833 (94%)]\tLoss: 1.380456\n",
      "Train Epoch: 2 [0/833 (0%)]\tLoss: 1.381900\n",
      "Train Epoch: 2 [200/833 (24%)]\tLoss: 1.383464\n",
      "Train Epoch: 2 [400/833 (47%)]\tLoss: 1.380546\n",
      "Train Epoch: 2 [600/833 (71%)]\tLoss: 1.380438\n",
      "Train Epoch: 2 [528/833 (94%)]\tLoss: 1.382645\n",
      "Train Epoch: 3 [0/833 (0%)]\tLoss: 1.376758\n",
      "Train Epoch: 3 [200/833 (24%)]\tLoss: 1.375383\n",
      "Train Epoch: 3 [400/833 (47%)]\tLoss: 1.373232\n",
      "Train Epoch: 3 [600/833 (71%)]\tLoss: 1.368253\n",
      "Train Epoch: 3 [528/833 (94%)]\tLoss: 1.378875\n",
      "Train Epoch: 4 [0/833 (0%)]\tLoss: 1.380345\n",
      "Train Epoch: 4 [200/833 (24%)]\tLoss: 1.383578\n",
      "Train Epoch: 4 [400/833 (47%)]\tLoss: 1.380634\n",
      "Train Epoch: 4 [600/833 (71%)]\tLoss: 1.375081\n",
      "Train Epoch: 4 [528/833 (94%)]\tLoss: 1.378607\n",
      "Train Epoch: 5 [0/833 (0%)]\tLoss: 1.380390\n",
      "Train Epoch: 5 [200/833 (24%)]\tLoss: 1.372285\n",
      "Train Epoch: 5 [400/833 (47%)]\tLoss: 1.371868\n",
      "Train Epoch: 5 [600/833 (71%)]\tLoss: 1.367621\n",
      "Train Epoch: 5 [528/833 (94%)]\tLoss: 1.365543\n",
      "Train Epoch: 6 [0/833 (0%)]\tLoss: 1.374625\n",
      "Train Epoch: 6 [200/833 (24%)]\tLoss: 1.370671\n",
      "Train Epoch: 6 [400/833 (47%)]\tLoss: 1.374111\n",
      "Train Epoch: 6 [600/833 (71%)]\tLoss: 1.378155\n",
      "Train Epoch: 6 [528/833 (94%)]\tLoss: 1.366521\n",
      "Train Epoch: 7 [0/833 (0%)]\tLoss: 1.365181\n",
      "Train Epoch: 7 [200/833 (24%)]\tLoss: 1.361760\n",
      "Train Epoch: 7 [400/833 (47%)]\tLoss: 1.387951\n",
      "Train Epoch: 7 [600/833 (71%)]\tLoss: 1.364640\n",
      "Train Epoch: 7 [528/833 (94%)]\tLoss: 1.373667\n",
      "Train Epoch: 8 [0/833 (0%)]\tLoss: 1.371203\n",
      "Train Epoch: 8 [200/833 (24%)]\tLoss: 1.365041\n",
      "Train Epoch: 8 [400/833 (47%)]\tLoss: 1.371867\n",
      "Train Epoch: 8 [600/833 (71%)]\tLoss: 1.374154\n",
      "Train Epoch: 8 [528/833 (94%)]\tLoss: 1.376871\n",
      "Train Epoch: 9 [0/833 (0%)]\tLoss: 1.370183\n",
      "Train Epoch: 9 [200/833 (24%)]\tLoss: 1.380384\n",
      "Train Epoch: 9 [400/833 (47%)]\tLoss: 1.342411\n",
      "Train Epoch: 9 [600/833 (71%)]\tLoss: 1.365583\n",
      "Train Epoch: 9 [528/833 (94%)]\tLoss: 1.379609\n",
      "Train Epoch: 10 [0/833 (0%)]\tLoss: 1.365922\n",
      "Train Epoch: 10 [200/833 (24%)]\tLoss: 1.357964\n",
      "Train Epoch: 10 [400/833 (47%)]\tLoss: 1.364447\n",
      "Train Epoch: 10 [600/833 (71%)]\tLoss: 1.365607\n",
      "Train Epoch: 10 [528/833 (94%)]\tLoss: 1.356122\n",
      "Train Epoch: 11 [0/833 (0%)]\tLoss: 1.346343\n",
      "Train Epoch: 11 [200/833 (24%)]\tLoss: 1.383698\n",
      "Train Epoch: 11 [400/833 (47%)]\tLoss: 1.360186\n",
      "Train Epoch: 11 [600/833 (71%)]\tLoss: 1.369312\n",
      "Train Epoch: 11 [528/833 (94%)]\tLoss: 1.399672\n",
      "Train Epoch: 12 [0/833 (0%)]\tLoss: 1.360122\n",
      "Train Epoch: 12 [200/833 (24%)]\tLoss: 1.372351\n",
      "Train Epoch: 12 [400/833 (47%)]\tLoss: 1.385984\n",
      "Train Epoch: 12 [600/833 (71%)]\tLoss: 1.339536\n",
      "Train Epoch: 12 [528/833 (94%)]\tLoss: 1.374655\n",
      "Train Epoch: 13 [0/833 (0%)]\tLoss: 1.343207\n",
      "Train Epoch: 13 [200/833 (24%)]\tLoss: 1.386991\n",
      "Train Epoch: 13 [400/833 (47%)]\tLoss: 1.355306\n",
      "Train Epoch: 13 [600/833 (71%)]\tLoss: 1.359314\n",
      "Train Epoch: 13 [528/833 (94%)]\tLoss: 1.369826\n",
      "Train Epoch: 14 [0/833 (0%)]\tLoss: 1.357347\n",
      "Train Epoch: 14 [200/833 (24%)]\tLoss: 1.347556\n",
      "Train Epoch: 14 [400/833 (47%)]\tLoss: 1.358576\n",
      "Train Epoch: 14 [600/833 (71%)]\tLoss: 1.369482\n",
      "Train Epoch: 14 [528/833 (94%)]\tLoss: 1.352106\n",
      "Train Epoch: 15 [0/833 (0%)]\tLoss: 1.369626\n",
      "Train Epoch: 15 [200/833 (24%)]\tLoss: 1.364176\n",
      "Train Epoch: 15 [400/833 (47%)]\tLoss: 1.364537\n",
      "Train Epoch: 15 [600/833 (71%)]\tLoss: 1.327901\n",
      "Train Epoch: 15 [528/833 (94%)]\tLoss: 1.327466\n",
      "Train Epoch: 16 [0/833 (0%)]\tLoss: 1.376500\n",
      "Train Epoch: 16 [200/833 (24%)]\tLoss: 1.342501\n",
      "Train Epoch: 16 [400/833 (47%)]\tLoss: 1.352918\n",
      "Train Epoch: 16 [600/833 (71%)]\tLoss: 1.366985\n",
      "Train Epoch: 16 [528/833 (94%)]\tLoss: 1.340578\n",
      "Train Epoch: 17 [0/833 (0%)]\tLoss: 1.364383\n",
      "Train Epoch: 17 [200/833 (24%)]\tLoss: 1.361033\n",
      "Train Epoch: 17 [400/833 (47%)]\tLoss: 1.391760\n",
      "Train Epoch: 17 [600/833 (71%)]\tLoss: 1.350347\n",
      "Train Epoch: 17 [528/833 (94%)]\tLoss: 1.352077\n",
      "Train Epoch: 18 [0/833 (0%)]\tLoss: 1.367476\n",
      "Train Epoch: 18 [200/833 (24%)]\tLoss: 1.372594\n",
      "Train Epoch: 18 [400/833 (47%)]\tLoss: 1.337099\n",
      "Train Epoch: 18 [600/833 (71%)]\tLoss: 1.367479\n",
      "Train Epoch: 18 [528/833 (94%)]\tLoss: 1.362258\n",
      "Train Epoch: 19 [0/833 (0%)]\tLoss: 1.359361\n",
      "Train Epoch: 19 [200/833 (24%)]\tLoss: 1.358420\n",
      "Train Epoch: 19 [400/833 (47%)]\tLoss: 1.333070\n",
      "Train Epoch: 19 [600/833 (71%)]\tLoss: 1.374468\n",
      "Train Epoch: 19 [528/833 (94%)]\tLoss: 1.342284\n",
      "Train Epoch: 20 [0/833 (0%)]\tLoss: 1.361729\n",
      "Train Epoch: 20 [200/833 (24%)]\tLoss: 1.302578\n",
      "Train Epoch: 20 [400/833 (47%)]\tLoss: 1.348933\n",
      "Train Epoch: 20 [600/833 (71%)]\tLoss: 1.326968\n",
      "Train Epoch: 20 [528/833 (94%)]\tLoss: 1.355738\n",
      "Train Epoch: 21 [0/833 (0%)]\tLoss: 1.363116\n",
      "Train Epoch: 21 [200/833 (24%)]\tLoss: 1.375112\n",
      "Train Epoch: 21 [400/833 (47%)]\tLoss: 1.318921\n",
      "Train Epoch: 21 [600/833 (71%)]\tLoss: 1.372992\n",
      "Train Epoch: 21 [528/833 (94%)]\tLoss: 1.308214\n",
      "Train Epoch: 22 [0/833 (0%)]\tLoss: 1.305334\n",
      "Train Epoch: 22 [200/833 (24%)]\tLoss: 1.290422\n",
      "Train Epoch: 22 [400/833 (47%)]\tLoss: 1.362295\n",
      "Train Epoch: 22 [600/833 (71%)]\tLoss: 1.288653\n",
      "Train Epoch: 22 [528/833 (94%)]\tLoss: 1.363556\n",
      "Train Epoch: 23 [0/833 (0%)]\tLoss: 1.332768\n",
      "Train Epoch: 23 [200/833 (24%)]\tLoss: 1.351004\n",
      "Train Epoch: 23 [400/833 (47%)]\tLoss: 1.328254\n",
      "Train Epoch: 23 [600/833 (71%)]\tLoss: 1.342880\n",
      "Train Epoch: 23 [528/833 (94%)]\tLoss: 1.314138\n",
      "Train Epoch: 24 [0/833 (0%)]\tLoss: 1.310450\n",
      "Train Epoch: 24 [200/833 (24%)]\tLoss: 1.337116\n",
      "Train Epoch: 24 [400/833 (47%)]\tLoss: 1.290804\n",
      "Train Epoch: 24 [600/833 (71%)]\tLoss: 1.338444\n",
      "Train Epoch: 24 [528/833 (94%)]\tLoss: 1.332459\n",
      "Train Epoch: 25 [0/833 (0%)]\tLoss: 1.396533\n",
      "Train Epoch: 25 [200/833 (24%)]\tLoss: 1.314060\n",
      "Train Epoch: 25 [400/833 (47%)]\tLoss: 1.311388\n",
      "Train Epoch: 25 [600/833 (71%)]\tLoss: 1.315728\n",
      "Train Epoch: 25 [528/833 (94%)]\tLoss: 1.318564\n",
      "Train Epoch: 26 [0/833 (0%)]\tLoss: 1.312634\n",
      "Train Epoch: 26 [200/833 (24%)]\tLoss: 1.299875\n",
      "Train Epoch: 26 [400/833 (47%)]\tLoss: 1.354703\n",
      "Train Epoch: 26 [600/833 (71%)]\tLoss: 1.353210\n",
      "Train Epoch: 26 [528/833 (94%)]\tLoss: 1.387457\n",
      "Train Epoch: 27 [0/833 (0%)]\tLoss: 1.322831\n",
      "Train Epoch: 27 [200/833 (24%)]\tLoss: 1.350704\n",
      "Train Epoch: 27 [400/833 (47%)]\tLoss: 1.312718\n",
      "Train Epoch: 27 [600/833 (71%)]\tLoss: 1.291356\n",
      "Train Epoch: 27 [528/833 (94%)]\tLoss: 1.325108\n",
      "Train Epoch: 28 [0/833 (0%)]\tLoss: 1.318923\n",
      "Train Epoch: 28 [200/833 (24%)]\tLoss: 1.280245\n",
      "Train Epoch: 28 [400/833 (47%)]\tLoss: 1.346621\n",
      "Train Epoch: 28 [600/833 (71%)]\tLoss: 1.323210\n",
      "Train Epoch: 28 [528/833 (94%)]\tLoss: 1.240142\n",
      "Train Epoch: 29 [0/833 (0%)]\tLoss: 1.341174\n",
      "Train Epoch: 29 [200/833 (24%)]\tLoss: 1.313615\n",
      "Train Epoch: 29 [400/833 (47%)]\tLoss: 1.275040\n",
      "Train Epoch: 29 [600/833 (71%)]\tLoss: 1.356144\n",
      "Train Epoch: 29 [528/833 (94%)]\tLoss: 1.339373\n",
      "Train Epoch: 30 [0/833 (0%)]\tLoss: 1.422807\n",
      "Train Epoch: 30 [200/833 (24%)]\tLoss: 1.356901\n",
      "Train Epoch: 30 [400/833 (47%)]\tLoss: 1.364464\n",
      "Train Epoch: 30 [600/833 (71%)]\tLoss: 1.327006\n",
      "Train Epoch: 30 [528/833 (94%)]\tLoss: 1.281081\n",
      "Train Epoch: 31 [0/833 (0%)]\tLoss: 1.283151\n",
      "Train Epoch: 31 [200/833 (24%)]\tLoss: 1.367383\n",
      "Train Epoch: 31 [400/833 (47%)]\tLoss: 1.241672\n",
      "Train Epoch: 31 [600/833 (71%)]\tLoss: 1.299230\n",
      "Train Epoch: 31 [528/833 (94%)]\tLoss: 1.262299\n",
      "Train Epoch: 32 [0/833 (0%)]\tLoss: 1.430454\n",
      "Train Epoch: 32 [200/833 (24%)]\tLoss: 1.272336\n",
      "Train Epoch: 32 [400/833 (47%)]\tLoss: 1.302708\n",
      "Train Epoch: 32 [600/833 (71%)]\tLoss: 1.251816\n",
      "Train Epoch: 32 [528/833 (94%)]\tLoss: 1.289994\n",
      "Train Epoch: 33 [0/833 (0%)]\tLoss: 1.329876\n",
      "Train Epoch: 33 [200/833 (24%)]\tLoss: 1.307720\n",
      "Train Epoch: 33 [400/833 (47%)]\tLoss: 1.359203\n",
      "Train Epoch: 33 [600/833 (71%)]\tLoss: 1.199131\n",
      "Train Epoch: 33 [528/833 (94%)]\tLoss: 1.416845\n",
      "Train Epoch: 34 [0/833 (0%)]\tLoss: 1.338519\n",
      "Train Epoch: 34 [200/833 (24%)]\tLoss: 1.270389\n",
      "Train Epoch: 34 [400/833 (47%)]\tLoss: 1.317752\n",
      "Train Epoch: 34 [600/833 (71%)]\tLoss: 1.376989\n",
      "Train Epoch: 34 [528/833 (94%)]\tLoss: 1.314291\n",
      "Train Epoch: 35 [0/833 (0%)]\tLoss: 1.449977\n",
      "Train Epoch: 35 [200/833 (24%)]\tLoss: 1.398687\n",
      "Train Epoch: 35 [400/833 (47%)]\tLoss: 1.322339\n",
      "Train Epoch: 35 [600/833 (71%)]\tLoss: 1.297398\n",
      "Train Epoch: 35 [528/833 (94%)]\tLoss: 1.330602\n",
      "Train Epoch: 36 [0/833 (0%)]\tLoss: 1.292728\n",
      "Train Epoch: 36 [200/833 (24%)]\tLoss: 1.216915\n",
      "Train Epoch: 36 [400/833 (47%)]\tLoss: 1.371898\n",
      "Train Epoch: 36 [600/833 (71%)]\tLoss: 1.181653\n",
      "Train Epoch: 36 [528/833 (94%)]\tLoss: 1.346118\n",
      "Train Epoch: 37 [0/833 (0%)]\tLoss: 1.328795\n",
      "Train Epoch: 37 [200/833 (24%)]\tLoss: 1.279226\n",
      "Train Epoch: 37 [400/833 (47%)]\tLoss: 1.350447\n",
      "Train Epoch: 37 [600/833 (71%)]\tLoss: 1.338645\n",
      "Train Epoch: 37 [528/833 (94%)]\tLoss: 1.456899\n",
      "Train Epoch: 38 [0/833 (0%)]\tLoss: 1.283798\n",
      "Train Epoch: 38 [200/833 (24%)]\tLoss: 1.237237\n",
      "Train Epoch: 38 [400/833 (47%)]\tLoss: 1.284653\n",
      "Train Epoch: 38 [600/833 (71%)]\tLoss: 1.378485\n",
      "Train Epoch: 38 [528/833 (94%)]\tLoss: 1.429532\n",
      "Train Epoch: 39 [0/833 (0%)]\tLoss: 1.311993\n",
      "Train Epoch: 39 [200/833 (24%)]\tLoss: 1.337614\n",
      "Train Epoch: 39 [400/833 (47%)]\tLoss: 1.349779\n",
      "Train Epoch: 39 [600/833 (71%)]\tLoss: 1.259624\n",
      "Train Epoch: 39 [528/833 (94%)]\tLoss: 1.404543\n",
      "Train Epoch: 40 [0/833 (0%)]\tLoss: 1.266339\n",
      "Train Epoch: 40 [200/833 (24%)]\tLoss: 1.321881\n",
      "Train Epoch: 40 [400/833 (47%)]\tLoss: 1.286339\n",
      "Train Epoch: 40 [600/833 (71%)]\tLoss: 1.315057\n",
      "Train Epoch: 40 [528/833 (94%)]\tLoss: 1.362757\n",
      "Train Epoch: 41 [0/833 (0%)]\tLoss: 1.326275\n",
      "Train Epoch: 41 [200/833 (24%)]\tLoss: 1.366146\n",
      "Train Epoch: 41 [400/833 (47%)]\tLoss: 1.237594\n",
      "Train Epoch: 41 [600/833 (71%)]\tLoss: 1.262310\n",
      "Train Epoch: 41 [528/833 (94%)]\tLoss: 1.331945\n",
      "Train Epoch: 42 [0/833 (0%)]\tLoss: 1.329901\n",
      "Train Epoch: 42 [200/833 (24%)]\tLoss: 1.278833\n",
      "Train Epoch: 42 [400/833 (47%)]\tLoss: 1.331667\n",
      "Train Epoch: 42 [600/833 (71%)]\tLoss: 1.371265\n",
      "Train Epoch: 42 [528/833 (94%)]\tLoss: 1.227259\n",
      "Train Epoch: 43 [0/833 (0%)]\tLoss: 1.337223\n",
      "Train Epoch: 43 [200/833 (24%)]\tLoss: 1.370480\n",
      "Train Epoch: 43 [400/833 (47%)]\tLoss: 1.312658\n",
      "Train Epoch: 43 [600/833 (71%)]\tLoss: 1.434454\n",
      "Train Epoch: 43 [528/833 (94%)]\tLoss: 1.235154\n",
      "Train Epoch: 44 [0/833 (0%)]\tLoss: 1.305272\n",
      "Train Epoch: 44 [200/833 (24%)]\tLoss: 1.269222\n",
      "Train Epoch: 44 [400/833 (47%)]\tLoss: 1.299743\n",
      "Train Epoch: 44 [600/833 (71%)]\tLoss: 1.269974\n",
      "Train Epoch: 44 [528/833 (94%)]\tLoss: 1.276947\n",
      "Train Epoch: 45 [0/833 (0%)]\tLoss: 1.400751\n",
      "Train Epoch: 45 [200/833 (24%)]\tLoss: 1.248270\n",
      "Train Epoch: 45 [400/833 (47%)]\tLoss: 1.344959\n",
      "Train Epoch: 45 [600/833 (71%)]\tLoss: 1.408363\n",
      "Train Epoch: 45 [528/833 (94%)]\tLoss: 1.354594\n",
      "Train Epoch: 46 [0/833 (0%)]\tLoss: 1.368820\n",
      "Train Epoch: 46 [200/833 (24%)]\tLoss: 1.443503\n",
      "Train Epoch: 46 [400/833 (47%)]\tLoss: 1.244880\n",
      "Train Epoch: 46 [600/833 (71%)]\tLoss: 1.297684\n",
      "Train Epoch: 46 [528/833 (94%)]\tLoss: 1.319559\n",
      "Train Epoch: 47 [0/833 (0%)]\tLoss: 1.398704\n",
      "Train Epoch: 47 [200/833 (24%)]\tLoss: 1.206189\n",
      "Train Epoch: 47 [400/833 (47%)]\tLoss: 1.372658\n",
      "Train Epoch: 47 [600/833 (71%)]\tLoss: 1.401065\n",
      "Train Epoch: 47 [528/833 (94%)]\tLoss: 1.244475\n",
      "Train Epoch: 48 [0/833 (0%)]\tLoss: 1.369871\n",
      "Train Epoch: 48 [200/833 (24%)]\tLoss: 1.280102\n",
      "Train Epoch: 48 [400/833 (47%)]\tLoss: 1.360965\n",
      "Train Epoch: 48 [600/833 (71%)]\tLoss: 1.272064\n",
      "Train Epoch: 48 [528/833 (94%)]\tLoss: 1.283952\n",
      "Train Epoch: 49 [0/833 (0%)]\tLoss: 1.234698\n",
      "Train Epoch: 49 [200/833 (24%)]\tLoss: 1.312557\n",
      "Train Epoch: 49 [400/833 (47%)]\tLoss: 1.342852\n",
      "Train Epoch: 49 [600/833 (71%)]\tLoss: 1.265543\n",
      "Train Epoch: 49 [528/833 (94%)]\tLoss: 1.332290\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "tensor([0, 1, 1, 0, 0, 2, 1, 0, 3, 2, 1, 1, 3, 1, 3, 0, 0, 0, 0, 0, 1, 0, 2, 1,\n",
      "        1, 3, 0, 2, 2, 3, 1, 3, 3, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 3, 1, 1, 0,\n",
      "        1, 0])\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "tensor([2, 1, 0, 1, 2, 1, 1, 1, 1, 2, 0, 1, 1, 1, 0, 3, 1, 2, 1, 0, 1, 1, 0, 0,\n",
      "        1, 2, 2, 2, 3, 1, 2, 1, 1, 0, 1, 2, 3, 2, 1, 1, 0, 3, 0, 1, 0, 0, 3, 3,\n",
      "        3, 1])\n",
      "[1 1 1 1]\n",
      "tensor([2, 0, 0, 2])\n",
      "Ending accuracy =  39.42\n"
     ]
    }
   ],
   "source": [
    "train_folder = './dataset/train_unbiased'\n",
    "test_folder = './dataset/test_unbiased'\n",
    "validate_folder = './dataset/validate_unbiased'\n",
    "\n",
    "class_weights = [0.3, 0.1, 0.3, 0.3]  # Example weights for each class\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[119.93560047938638, 121.99074304889741, 129.42976558005753], std=[65.61636024791385, 64.00107977894356, 60.628164585048744]),  # Normalize images\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_folder, transform=transform)\n",
    "weights = [class_weights[label] for label in train_dataset.targets]\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_folder, transform=transform)\n",
    "weights = [class_weights[label] for label in test_dataset.targets]\n",
    "test_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "validate_dataset = datasets.ImageFolder(validate_folder, transform=transform)\n",
    "weights = [class_weights[label] for label in validate_dataset.targets]\n",
    "valid_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "\n",
    "# number of epoch and log interval reduced for testing\n",
    "hyperparameters = {\n",
    "    'n_epochs' : 50,\n",
    "    'batch_size' : 50,\n",
    "    'learning_rate' : 1e-3,\n",
    "    'momentum' : 0.5,\n",
    "    'log_interval' : 4\n",
    "}\n",
    "\n",
    "network = AlexNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "criterion_mean = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD(network.params(),\n",
    "                        lr=hyperparameters['learning_rate'],\n",
    "                        momentum=hyperparameters['momentum'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], sampler=train_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparameters['batch_size'], sampler=test_sampler)\n",
    "valid_loader = DataLoader(validate_dataset, batch_size=hyperparameters['batch_size'], sampler=valid_sampler)\n",
    "\n",
    "our_model = Reweighting(network, hyperparameters, criterion, criterion_mean, optimizer, train_loader, valid_loader, test_loader)\n",
    "\n",
    "start_accuracy = our_model.test()\n",
    "print(\"Starting accuracy = \", start_accuracy)\n",
    "\n",
    "our_model.train()\n",
    "\n",
    "end_accuracy = our_model.test()\n",
    "print(\"Ending accuracy = \", end_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda:0\n",
      "Starting accuracy =  9.569999694824219\n",
      "Ending accuracy =  18.65999984741211\n"
     ]
    }
   ],
   "source": [
    "## Test control model\n",
    "# from main import Reweighting\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)\n",
    "\n",
    "class NoReweighting():\n",
    "    def __init__(self, network, hyperparameters, criterion, optimizer, train_loader, test_loader):\n",
    "        self.network = network\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.train_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "    def train(self):\n",
    "        # Train the network\n",
    "        for epoch in range(hyperparameters['n_epochs']):\n",
    "            self.network.train()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.network(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                # if batch_idx % hyperparameters['log_interval'] == 0:\n",
    "                #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                #         100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            self.network.eval()\n",
    "            output = self.network(data.to(device)).cpu()\n",
    "            test_loss += self.criterion(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "        # print('\\nTest set: \\nAvg. loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        #     test_loss, correct, len(test_loader.dataset),\n",
    "        #     100.0 * correct / len(test_loader.dataset)))\n",
    "        \n",
    "        return (100.0 * correct / len(test_loader.dataset)).item()\n",
    "\n",
    "# number of epoch and log interval reduced for testing\n",
    "hyperparameters = {\n",
    "    'n_epochs' : 2,\n",
    "    'batch_size_train' : 100,\n",
    "    'batch_size_valid' : 10,\n",
    "    'batch_size_test' : 1000,\n",
    "    'learning_rate' : 1e-3,\n",
    "    'momentum' : 0.5,\n",
    "    'log_interval' : 1\n",
    "}\n",
    "\n",
    "network = LeNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "criterion_mean = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD(network.params(),\n",
    "                        lr=hyperparameters['learning_rate'],\n",
    "                        momentum=hyperparameters['momentum'])\n",
    "# Load the data\n",
    "data_loader = MNISTDataLoader(validation_ratio=0.05,\n",
    "                            batch_size_train=hyperparameters['batch_size_train'],\n",
    "                            batch_size_valid=hyperparameters['batch_size_valid'],\n",
    "                            batch_size_test=hyperparameters['batch_size_test'])\n",
    "\n",
    "# 10% representation by 9's class in training data\n",
    "desired_sample_distribution = [100, 100, 100, 100, 100, 100, 100, 100, 100, 10]\n",
    "data_loader.sample_bias(desired_sample_distribution, dataset=\"train\")\n",
    "\n",
    "\n",
    "train_loader = data_loader.train_dataloader\n",
    "valid_loader = data_loader.valid_dataloader\n",
    "test_loader = data_loader.test_dataloader\n",
    "\n",
    "\n",
    "our_model = NoReweighting(network, hyperparameters, criterion_mean, optimizer, train_loader, test_loader)\n",
    "\n",
    "start_accuracy = our_model.test()\n",
    "print(\"Starting accuracy = \", start_accuracy)\n",
    "\n",
    "our_model.train()\n",
    "\n",
    "end_accuracy = our_model.test()\n",
    "print(\"Ending accuracy = \", end_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE50024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
