{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from model import AlexNet, LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Test learning to reweight model\n",
    "# from main import Reweighting\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)\n",
    "\n",
    "class Reweighting():\n",
    "    def __init__(self, network, hyperparameters, criterion, criterion_mean, optimizer, train_loader, valid_loader, test_loader):\n",
    "        self.network = network.requires_grad_(requires_grad=True)\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion_mean = criterion_mean\n",
    "        self.gradient_network = None\n",
    "    \n",
    "    def paper_train(self):\n",
    "        # X_g = LeNet5()\n",
    "        # print(\"Starting training...\")\n",
    "        X_g, y_g = next(iter(self.valid_loader))\n",
    "        X_g = X_g.to(device)\n",
    "        y_g = y_g.to(device)\n",
    "\n",
    "        y_f_hat = torch.empty(1)\n",
    "        y_f_hat = y_f_hat.to(device)\n",
    "\n",
    "        theta_tp1 = self.network.state_dict()\n",
    "        for epoch in range(self.hyperparameters['n_epochs']):\n",
    "            self.network.train()\n",
    "\n",
    "            self.gradient_network = AlexNet()\n",
    "            self.gradient_network.load_state_dict(theta_tp1)\n",
    "\n",
    "            # get batch of data from train_loader\n",
    "            X_f, y_f = next(iter(self.train_loader))\n",
    "            X_f = X_f.to(device)\n",
    "            y_f = y_f.to(device)\n",
    "\n",
    "            # Line 4\n",
    "            y_f_hat = self.gradient_network(X_f)\n",
    "            \n",
    "            # Line 5\n",
    "            epsilon = torch.zeros(y_f.size(), requires_grad=True)\n",
    "            epsilon = epsilon.to(device)\n",
    "\n",
    "            Costs = self.criterion(y_f_hat, y_f.float())\n",
    "            l_f = torch.sum(torch.mul(Costs, epsilon))\n",
    "\n",
    "            # Line 6\n",
    "            grad_t = torch.autograd.grad(outputs=l_f, inputs=self.gradient_network.params(), create_graph=True)\n",
    "            \n",
    "\n",
    "            # Line 7: manually update the weights of the validation network\n",
    "            lr = self.hyperparameters['learning_rate']\n",
    "            self.gradient_network.update_params_SGD_step(lr, grad_t)\n",
    "\n",
    "            # Line 8\n",
    "            # Model has theta_hat\n",
    "            y_g_hat = self.gradient_network(X_g)\n",
    "\n",
    "            # Line 9\n",
    "            l_g = self.criterion_mean(y_g_hat, y_g.float())\n",
    "\n",
    "            # Line 10\n",
    "            grad_epsilon = torch.autograd.grad(l_g, epsilon, only_inputs=True)[0]\n",
    "\n",
    "            # Line 11\n",
    "            w_tilde = torch.clamp(-grad_epsilon, min=0)\n",
    "\n",
    "            if torch.sum(w_tilde) != 0:\n",
    "                w = w_tilde / torch.sum(w_tilde)\n",
    "            else:\n",
    "                w = w_tilde\n",
    "\n",
    "            # Line 12\n",
    "            y_f_hat = self.network(X_f)\n",
    "            Costs = self.criterion(y_f_hat, y_f.float())\n",
    "            l_f_hat = torch.sum(torch.mul(Costs, w))\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Line 13\n",
    "            l_f_hat.backward()\n",
    "\n",
    "            # Line 14\n",
    "            self.optimizer.step()\n",
    "            # break\n",
    "\n",
    "            theta_tp1 = self.network.state_dict()\n",
    "\n",
    "            if (epoch % self.hyperparameters['log_interval'] == 0):\n",
    "                curr_accuracy = self.test()\n",
    "                print(\"Epoch \" + str(epoch) + \" accuracy = \", curr_accuracy)\n",
    "\n",
    "    def train(self):\n",
    "        # Train the network\n",
    "        for epoch in range(self.hyperparameters['n_epochs']):\n",
    "            self.network.train()\n",
    "            for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.network(data)\n",
    "                # print(output)\n",
    "                # print(target.float())\n",
    "                loss = self.criterion_mean(output, target.float())\n",
    "                # print(loss)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if batch_idx % self.hyperparameters['log_interval'] == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(self.train_loader.dataset),\n",
    "                        100. * batch_idx / len(self.train_loader), loss.item()))\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        self.network.eval()\n",
    "\n",
    "        acc = np.array([])\n",
    "        for itr,(test_img, test_label) in enumerate(self.test_loader):\n",
    "            prediction = self.network(test_img.to(device)).detach().cpu()\n",
    "            # print(prediction)\n",
    "            prediction = (torch.sigmoid(prediction) > 0.5).int().numpy()\n",
    "            tmp = (prediction == test_label.detach().numpy())\n",
    "            # print(prediction)\n",
    "            # print(test_label)\n",
    "            # print(tmp)\n",
    "            # acc.append(tmp)\n",
    "            acc = np.append(acc,tmp)\n",
    "\n",
    "        # print(acc)\n",
    "        accuracy = np.mean(acc)\n",
    "        return np.round(accuracy*100,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting accuracy =  52.1\n",
      "Epoch 0 accuracy =  49.6\n",
      "Epoch 5 accuracy =  48.3\n",
      "Epoch 10 accuracy =  52.9\n",
      "Epoch 15 accuracy =  51.1\n",
      "Epoch 20 accuracy =  48.3\n",
      "Epoch 25 accuracy =  52.3\n",
      "Epoch 30 accuracy =  49.3\n",
      "Epoch 35 accuracy =  48.1\n",
      "Epoch 40 accuracy =  48.9\n",
      "Epoch 45 accuracy =  49.2\n",
      "Epoch 50 accuracy =  48.6\n",
      "Epoch 55 accuracy =  50.9\n",
      "Epoch 60 accuracy =  51.1\n",
      "Epoch 65 accuracy =  52.7\n",
      "Epoch 70 accuracy =  49.5\n",
      "Epoch 75 accuracy =  50.2\n",
      "Epoch 80 accuracy =  48.8\n",
      "Epoch 85 accuracy =  48.8\n",
      "Epoch 90 accuracy =  49.8\n",
      "Epoch 95 accuracy =  50.6\n",
      "Ending accuracy =  47.8\n"
     ]
    }
   ],
   "source": [
    "# train_folder = '../dataset-ninja/train_unbiased'\n",
    "# test_folder = '../dataset-ninja/test_unbiased'\n",
    "# validate_folder = '../dataset-ninja/validate_unbiased'\n",
    "train_folder = '../dataset-ninja/train_binary'\n",
    "test_folder = '../dataset-ninja/test_binary'\n",
    "validate_folder = '../dataset-ninja/validate_binary'\n",
    "\n",
    "class_weights = [0.5, 0.5]  # Example weights for each class\n",
    "equal_weights = [0.5, 0.5]  # Equal weighting used for testing/validataion datasets\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[90.2867748375, 88.24014045, 94.9276596], std=[46.843379357493816, 46.59302413225906, 47.86539694070785]),  # Normalize images\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_folder, transform=transform)\n",
    "weights = [class_weights[label] for label in train_dataset.targets]\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_folder, transform=transform)\n",
    "weights = [equal_weights[label] for label in test_dataset.targets]\n",
    "test_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "validate_dataset = datasets.ImageFolder(validate_folder, transform=transform)\n",
    "weights = [equal_weights[label] for label in validate_dataset.targets]\n",
    "valid_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "\n",
    "# number of epoch and log interval reduced for testing\n",
    "hyperparameters = {\n",
    "    'n_epochs' : 100,\n",
    "    'batch_size' : 128,\n",
    "    'learning_rate' : 1e-3,\n",
    "    'momentum' : 0.5,\n",
    "    'log_interval' : 5\n",
    "}\n",
    "\n",
    "network = AlexNet()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "# criterion_mean = nn.CrossEntropyLoss(reduction='mean')\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "criterion_mean = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD(network.params(),\n",
    "                        lr=hyperparameters['learning_rate'],\n",
    "                        momentum=hyperparameters['momentum'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], sampler=train_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparameters['batch_size'], sampler=test_sampler)\n",
    "valid_loader = DataLoader(validate_dataset, batch_size=hyperparameters['batch_size'], sampler=valid_sampler)\n",
    "\n",
    "our_model = Reweighting(network, hyperparameters, criterion, criterion_mean, optimizer, train_loader, valid_loader, test_loader)\n",
    "\n",
    "start_accuracy = our_model.test()\n",
    "print(\"Starting accuracy = \", start_accuracy)\n",
    "\n",
    "# our_model.train()\n",
    "our_model.paper_train()\n",
    "\n",
    "end_accuracy = our_model.test()\n",
    "print(\"Ending accuracy = \", end_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 2, 2, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 7, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:03,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 7,  ..., 2, 2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:05,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([6, 2, 2,  ..., 2, 7, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:06,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 7,  ..., 4, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:07,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:08,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 4,  ..., 2, 7, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:09,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 6, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:10,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 7, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:12,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 2, 2, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:13,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:14,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([4, 2, 2,  ..., 4, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:15,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 2,  ..., 2, 2, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:16,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 7, 7,  ..., 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:18,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 7,  ..., 2, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:19,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 4,  ..., 4, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:20,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([4, 7, 2,  ..., 6, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:21,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 6, 7,  ..., 2, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:22,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 6,  ..., 2, 4, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:24,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 2,  ..., 7, 2, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:25,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 6, 2,  ..., 2, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:26,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:27,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 7,  ..., 6, 2, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:27,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "tensor([2, 2, 2, 6, 7, 2, 7, 6, 2, 2, 2, 2, 7, 1, 7, 2, 4, 2, 2, 2, 7, 7, 2, 7,\n",
      "        2, 7, 7, 4, 2, 7, 2, 7, 2, 2, 2, 4, 7, 7, 2, 6, 6, 7, 2, 2, 7, 2, 2, 7,\n",
      "        6, 2, 2, 2, 7, 7, 7, 7, 7, 1, 6, 2, 2, 2, 2, 4, 2, 7, 2, 7, 7, 7, 2, 2,\n",
      "        2, 2, 1, 2, 2, 2, 2, 7, 2, 2, 6, 2, 2, 2, 2, 2, 7, 2, 7, 2, 2, 7, 2, 2,\n",
      "        2, 2, 2, 7, 2, 2, 2, 7, 4, 7, 2, 7, 2, 7, 7, 2, 2, 4, 2, 2, 2, 6, 7, 2,\n",
      "        2, 2, 2, 2, 7, 0, 2, 2, 2, 1, 2, 7, 2, 2, 2, 7, 2, 2, 7, 2, 7, 2, 2, 2,\n",
      "        7, 6, 4, 7, 7, 2, 2, 2, 4, 2, 4, 0, 7, 6, 2, 2, 7, 2, 7, 6, 2, 2, 2, 2,\n",
      "        2, 6, 7, 2, 7, 2, 2, 2, 2, 7, 2, 7, 7, 4, 7, 2, 7, 7, 2, 2, 7, 7, 2, 1,\n",
      "        1, 7, 4, 7, 4, 4, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2,\n",
      "        7, 4, 7, 2, 7, 6, 2, 2, 7, 2, 2, 7, 2, 3, 1, 2, 7, 2, 2, 7, 4, 2, 4, 2,\n",
      "        7, 2, 2, 2, 7, 4, 7, 2, 4, 7, 2, 2, 2, 2, 2, 7, 4, 2, 7, 2, 7, 7, 2, 4,\n",
      "        2, 7, 2, 2, 2, 2, 7, 2, 2, 7, 7, 2, 2, 2, 2, 4, 0, 2, 3, 2, 2, 2, 2, 2,\n",
      "        2, 7, 2, 2, 4, 2, 1, 7, 4, 7, 2, 2, 2, 6, 2, 7, 7, 7, 2, 2])\n",
      "Ending accuracy =  56.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_model.network.eval()\n",
    "\n",
    "acc = []\n",
    "for itr,(test_img, test_label) in tqdm(enumerate(our_model.test_loader)):\n",
    "    prediction = our_model.network(test_img.to(device)).detach().cpu().numpy()\n",
    "    # print(prediction)\n",
    "    prediction = np.argmax(prediction, axis=1)\n",
    "    tmp = (prediction == test_label.detach().numpy())\n",
    "    print(prediction)\n",
    "    print(test_label)\n",
    "    acc.append(tmp)\n",
    "\n",
    "accuracy = np.concatenate(acc).mean()\n",
    "end_accuracy = np.round(accuracy*100,2)\n",
    "\n",
    "print(\"Ending accuracy = \", end_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda:0\n",
      "Starting accuracy =  9.569999694824219\n",
      "Ending accuracy =  18.65999984741211\n"
     ]
    }
   ],
   "source": [
    "## Test control model\n",
    "# from main import Reweighting\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)\n",
    "\n",
    "class NoReweighting():\n",
    "    def __init__(self, network, hyperparameters, criterion, optimizer, train_loader, test_loader):\n",
    "        self.network = network\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.train_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "    def train(self):\n",
    "        # Train the network\n",
    "        for epoch in range(hyperparameters['n_epochs']):\n",
    "            self.network.train()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.network(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                # if batch_idx % hyperparameters['log_interval'] == 0:\n",
    "                #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                #         100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            self.network.eval()\n",
    "            output = self.network(data.to(device)).cpu()\n",
    "            test_loss += self.criterion(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "        # print('\\nTest set: \\nAvg. loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        #     test_loss, correct, len(test_loader.dataset),\n",
    "        #     100.0 * correct / len(test_loader.dataset)))\n",
    "        \n",
    "        return (100.0 * correct / len(test_loader.dataset)).item()\n",
    "\n",
    "# number of epoch and log interval reduced for testing\n",
    "hyperparameters = {\n",
    "    'n_epochs' : 2,\n",
    "    'batch_size_train' : 100,\n",
    "    'batch_size_valid' : 10,\n",
    "    'batch_size_test' : 1000,\n",
    "    'learning_rate' : 1e-3,\n",
    "    'momentum' : 0.5,\n",
    "    'log_interval' : 1\n",
    "}\n",
    "\n",
    "network = LeNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "criterion_mean = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD(network.params(),\n",
    "                        lr=hyperparameters['learning_rate'],\n",
    "                        momentum=hyperparameters['momentum'])\n",
    "# Load the data\n",
    "data_loader = MNISTDataLoader(validation_ratio=0.05,\n",
    "                            batch_size_train=hyperparameters['batch_size_train'],\n",
    "                            batch_size_valid=hyperparameters['batch_size_valid'],\n",
    "                            batch_size_test=hyperparameters['batch_size_test'])\n",
    "\n",
    "# 10% representation by 9's class in training data\n",
    "desired_sample_distribution = [100, 100, 100, 100, 100, 100, 100, 100, 100, 10]\n",
    "data_loader.sample_bias(desired_sample_distribution, dataset=\"train\")\n",
    "\n",
    "\n",
    "train_loader = data_loader.train_dataloader\n",
    "valid_loader = data_loader.valid_dataloader\n",
    "test_loader = data_loader.test_dataloader\n",
    "\n",
    "\n",
    "our_model = NoReweighting(network, hyperparameters, criterion_mean, optimizer, train_loader, test_loader)\n",
    "\n",
    "start_accuracy = our_model.test()\n",
    "print(\"Starting accuracy = \", start_accuracy)\n",
    "\n",
    "our_model.train()\n",
    "\n",
    "end_accuracy = our_model.test()\n",
    "print(\"Ending accuracy = \", end_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE50024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
