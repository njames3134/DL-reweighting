{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from model import AlexNet, LeNet, JustinNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Test learning to reweight model\n",
    "# from main import Reweighting\n",
    "\n",
    "\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)\n",
    "\n",
    "class Reweighting():\n",
    "    def __init__(self, network, hyperparameters, criterion, criterion_mean, optimizer, train_loader, valid_loader, test_loader):\n",
    "        self.network = network.requires_grad_(requires_grad=True)\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion_mean = criterion_mean\n",
    "        self.gradient_network = None\n",
    "    \n",
    "    def paper_train(self):\n",
    "        # X_g = LeNet5()\n",
    "        # print(\"Starting training...\")\n",
    "        X_g, y_g = next(iter(self.valid_loader))\n",
    "        X_g = X_g.to(device)\n",
    "        y_g = y_g.to(device)\n",
    "\n",
    "        y_f_hat = torch.empty(1)\n",
    "        y_f_hat = y_f_hat.to(device)\n",
    "\n",
    "        theta_tp1 = self.network.state_dict()\n",
    "        for epoch in range(self.hyperparameters['n_epochs']):\n",
    "            self.network.train()\n",
    "\n",
    "            self.gradient_network = AlexNet()\n",
    "            # self.gradient_network = LeNet()\n",
    "            self.gradient_network.load_state_dict(theta_tp1)\n",
    "\n",
    "            # get batch of data from train_loader\n",
    "            X_f, y_f = next(iter(self.train_loader))\n",
    "            X_f = X_f.to(device)\n",
    "            y_f = y_f.to(device)\n",
    "\n",
    "            # Line 4\n",
    "            y_f_hat = self.gradient_network(X_f)\n",
    "            \n",
    "            # Line 5\n",
    "            epsilon = torch.zeros(y_f.size(), requires_grad=True)\n",
    "            epsilon = epsilon.to(device)\n",
    "\n",
    "            Costs = self.criterion(y_f_hat, y_f.float())\n",
    "            l_f = torch.sum(torch.mul(Costs, epsilon))\n",
    "\n",
    "            # Line 6\n",
    "            grad_t = torch.autograd.grad(outputs=l_f, inputs=self.gradient_network.params(), create_graph=True)\n",
    "            \n",
    "\n",
    "            # Line 7: manually update the weights of the validation network\n",
    "            lr = self.hyperparameters['learning_rate']\n",
    "            self.gradient_network.update_params_SGD_step(lr, grad_t)\n",
    "\n",
    "            # Line 8\n",
    "            # Model has theta_hat\n",
    "            y_g_hat = self.gradient_network(X_g)\n",
    "\n",
    "            # Line 9\n",
    "            l_g = self.criterion_mean(y_g_hat, y_g.float())\n",
    "\n",
    "            # Line 10\n",
    "            grad_epsilon = torch.autograd.grad(l_g, epsilon, only_inputs=True)[0]\n",
    "\n",
    "            # Line 11\n",
    "            w_tilde = torch.clamp(-grad_epsilon, min=0)\n",
    "\n",
    "            if torch.sum(w_tilde) != 0:\n",
    "                w = w_tilde / torch.sum(w_tilde)\n",
    "            else:\n",
    "                w = w_tilde\n",
    "\n",
    "            # Line 12\n",
    "            y_f_hat = self.network(X_f)\n",
    "            Costs = self.criterion(y_f_hat, y_f.float())\n",
    "            l_f_hat = torch.sum(torch.mul(Costs, w))\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Line 13\n",
    "            l_f_hat.backward()\n",
    "\n",
    "            # Line 14\n",
    "            self.optimizer.step()\n",
    "            # break\n",
    "\n",
    "            theta_tp1 = self.network.state_dict()\n",
    "\n",
    "            if (epoch % self.hyperparameters['log_interval'] == 0):\n",
    "                curr_accuracy = self.test()\n",
    "                print(\"Epoch \" + str(epoch) + \" accuracy = \", curr_accuracy, \" loss = \", l_f_hat.item())\n",
    "\n",
    "    def train(self):\n",
    "        # Train the network\n",
    "        for epoch in range(self.hyperparameters['n_epochs']):\n",
    "            self.network.train()\n",
    "            for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.network(data)\n",
    "                # print(output)\n",
    "                # print(target.float())\n",
    "                loss = self.criterion_mean(output, target.float())\n",
    "                # print(loss)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if batch_idx % self.hyperparameters['log_interval'] == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(self.train_loader.dataset),\n",
    "                        100. * batch_idx / len(self.train_loader), loss.item()))\n",
    "            \n",
    "            if (epoch % self.hyperparameters['log_interval'] == 0):\n",
    "                curr_accuracy = self.test()\n",
    "                print(\"Epoch \" + str(epoch) + \" accuracy = \", curr_accuracy)\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        self.network.eval()\n",
    "\n",
    "        acc = np.array([])\n",
    "        for itr,(test_img, test_label) in enumerate(self.test_loader):\n",
    "            prediction = self.network(test_img.to(device)).detach().cpu()\n",
    "            # print(prediction)\n",
    "            prediction = (torch.sigmoid(prediction) > 0.5).int().numpy()\n",
    "            tmp = (prediction == test_label.detach().numpy())\n",
    "            print(prediction)\n",
    "            # print(test_label)\n",
    "            # print(tmp)\n",
    "            # acc.append(tmp)\n",
    "            acc = np.append(acc,tmp)\n",
    "\n",
    "        # print(acc)\n",
    "        accuracy = np.mean(acc)\n",
    "        return np.round(accuracy*100,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Starting accuracy =  50.0\n",
      "Train Epoch: 0 [0/6493 (0%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [10/6493 (0%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [20/6493 (0%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [30/6493 (0%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [40/6493 (1%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [50/6493 (1%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [60/6493 (1%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [70/6493 (1%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [80/6493 (1%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [90/6493 (1%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [100/6493 (2%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [110/6493 (2%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [120/6493 (2%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [130/6493 (2%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [140/6493 (2%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [150/6493 (2%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [160/6493 (2%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [170/6493 (3%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [180/6493 (3%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [190/6493 (3%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [200/6493 (3%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [210/6493 (3%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [220/6493 (3%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [230/6493 (4%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [240/6493 (4%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [250/6493 (4%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [260/6493 (4%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [270/6493 (4%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [280/6493 (4%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [290/6493 (4%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [300/6493 (5%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [310/6493 (5%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [320/6493 (5%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [330/6493 (5%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [340/6493 (5%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [350/6493 (5%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [360/6493 (6%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [370/6493 (6%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [380/6493 (6%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [390/6493 (6%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [400/6493 (6%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [410/6493 (6%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [420/6493 (6%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [430/6493 (7%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [440/6493 (7%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [450/6493 (7%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [460/6493 (7%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [470/6493 (7%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [480/6493 (7%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [490/6493 (8%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [500/6493 (8%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [510/6493 (8%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [520/6493 (8%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [530/6493 (8%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [540/6493 (8%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [550/6493 (8%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [560/6493 (9%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [570/6493 (9%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [580/6493 (9%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [590/6493 (9%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [600/6493 (9%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [610/6493 (9%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [620/6493 (10%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [630/6493 (10%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [640/6493 (10%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [650/6493 (10%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [660/6493 (10%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [670/6493 (10%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [680/6493 (10%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [690/6493 (11%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [700/6493 (11%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [710/6493 (11%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [720/6493 (11%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [730/6493 (11%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [740/6493 (11%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [750/6493 (12%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [760/6493 (12%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [770/6493 (12%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [780/6493 (12%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [790/6493 (12%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [800/6493 (12%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [810/6493 (12%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [820/6493 (13%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [830/6493 (13%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [840/6493 (13%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [850/6493 (13%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [860/6493 (13%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [870/6493 (13%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [880/6493 (14%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [890/6493 (14%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [900/6493 (14%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [910/6493 (14%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [920/6493 (14%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [930/6493 (14%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [940/6493 (14%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [950/6493 (15%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [960/6493 (15%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [970/6493 (15%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [980/6493 (15%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [990/6493 (15%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1000/6493 (15%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1010/6493 (16%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1020/6493 (16%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1030/6493 (16%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1040/6493 (16%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1050/6493 (16%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1060/6493 (16%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1070/6493 (16%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1080/6493 (17%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1090/6493 (17%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1100/6493 (17%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1110/6493 (17%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1120/6493 (17%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1130/6493 (17%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1140/6493 (18%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1150/6493 (18%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1160/6493 (18%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1170/6493 (18%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1180/6493 (18%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1190/6493 (18%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1200/6493 (18%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1210/6493 (19%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1220/6493 (19%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1230/6493 (19%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1240/6493 (19%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1250/6493 (19%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1260/6493 (19%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1270/6493 (20%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1280/6493 (20%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1290/6493 (20%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1300/6493 (20%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1310/6493 (20%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1320/6493 (20%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1330/6493 (20%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1340/6493 (21%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1350/6493 (21%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1360/6493 (21%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1370/6493 (21%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1380/6493 (21%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1390/6493 (21%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1400/6493 (22%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1410/6493 (22%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1420/6493 (22%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1430/6493 (22%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1440/6493 (22%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1450/6493 (22%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1460/6493 (22%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1470/6493 (23%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1480/6493 (23%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1490/6493 (23%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1500/6493 (23%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1510/6493 (23%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1520/6493 (23%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1530/6493 (24%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1540/6493 (24%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1550/6493 (24%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1560/6493 (24%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1570/6493 (24%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1580/6493 (24%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1590/6493 (24%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1600/6493 (25%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1610/6493 (25%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1620/6493 (25%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1630/6493 (25%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1640/6493 (25%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1650/6493 (25%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1660/6493 (26%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1670/6493 (26%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1680/6493 (26%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1690/6493 (26%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1700/6493 (26%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1710/6493 (26%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1720/6493 (26%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1730/6493 (27%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1740/6493 (27%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1750/6493 (27%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1760/6493 (27%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1770/6493 (27%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1780/6493 (27%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1790/6493 (28%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1800/6493 (28%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1810/6493 (28%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1820/6493 (28%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1830/6493 (28%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1840/6493 (28%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1850/6493 (28%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1860/6493 (29%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1870/6493 (29%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1880/6493 (29%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1890/6493 (29%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1900/6493 (29%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1910/6493 (29%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1920/6493 (30%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1930/6493 (30%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1940/6493 (30%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1950/6493 (30%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1960/6493 (30%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1970/6493 (30%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1980/6493 (30%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [1990/6493 (31%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2000/6493 (31%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2010/6493 (31%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2020/6493 (31%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2030/6493 (31%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2040/6493 (31%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2050/6493 (32%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2060/6493 (32%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2070/6493 (32%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2080/6493 (32%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2090/6493 (32%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2100/6493 (32%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2110/6493 (32%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2120/6493 (33%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2130/6493 (33%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2140/6493 (33%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2150/6493 (33%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2160/6493 (33%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2170/6493 (33%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2180/6493 (34%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2190/6493 (34%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2200/6493 (34%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2210/6493 (34%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2220/6493 (34%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2230/6493 (34%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2240/6493 (34%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2250/6493 (35%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2260/6493 (35%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2270/6493 (35%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2280/6493 (35%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2290/6493 (35%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2300/6493 (35%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2310/6493 (36%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2320/6493 (36%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2330/6493 (36%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2340/6493 (36%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2350/6493 (36%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2360/6493 (36%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2370/6493 (36%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2380/6493 (37%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2390/6493 (37%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2400/6493 (37%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2410/6493 (37%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2420/6493 (37%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2430/6493 (37%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2440/6493 (38%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2450/6493 (38%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2460/6493 (38%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2470/6493 (38%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2480/6493 (38%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2490/6493 (38%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2500/6493 (38%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2510/6493 (39%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2520/6493 (39%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2530/6493 (39%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2540/6493 (39%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2550/6493 (39%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2560/6493 (39%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2570/6493 (40%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2580/6493 (40%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2590/6493 (40%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2600/6493 (40%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2610/6493 (40%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2620/6493 (40%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2630/6493 (40%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2640/6493 (41%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2650/6493 (41%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2660/6493 (41%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2670/6493 (41%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2680/6493 (41%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2690/6493 (41%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2700/6493 (42%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2710/6493 (42%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2720/6493 (42%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2730/6493 (42%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2740/6493 (42%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2750/6493 (42%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2760/6493 (43%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2770/6493 (43%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2780/6493 (43%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2790/6493 (43%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2800/6493 (43%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2810/6493 (43%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2820/6493 (43%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2830/6493 (44%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2840/6493 (44%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2850/6493 (44%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2860/6493 (44%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2870/6493 (44%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2880/6493 (44%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2890/6493 (45%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2900/6493 (45%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2910/6493 (45%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2920/6493 (45%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2930/6493 (45%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2940/6493 (45%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2950/6493 (45%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2960/6493 (46%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2970/6493 (46%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2980/6493 (46%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [2990/6493 (46%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3000/6493 (46%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3010/6493 (46%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3020/6493 (47%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3030/6493 (47%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3040/6493 (47%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3050/6493 (47%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3060/6493 (47%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3070/6493 (47%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3080/6493 (47%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3090/6493 (48%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3100/6493 (48%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3110/6493 (48%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3120/6493 (48%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3130/6493 (48%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3140/6493 (48%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3150/6493 (49%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3160/6493 (49%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3170/6493 (49%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3180/6493 (49%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3190/6493 (49%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3200/6493 (49%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3210/6493 (49%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3220/6493 (50%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3230/6493 (50%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3240/6493 (50%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3250/6493 (50%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3260/6493 (50%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3270/6493 (50%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3280/6493 (51%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3290/6493 (51%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3300/6493 (51%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3310/6493 (51%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3320/6493 (51%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3330/6493 (51%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3340/6493 (51%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3350/6493 (52%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3360/6493 (52%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3370/6493 (52%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3380/6493 (52%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3390/6493 (52%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3400/6493 (52%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3410/6493 (53%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3420/6493 (53%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3430/6493 (53%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3440/6493 (53%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3450/6493 (53%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3460/6493 (53%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3470/6493 (53%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3480/6493 (54%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3490/6493 (54%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3500/6493 (54%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3510/6493 (54%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3520/6493 (54%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3530/6493 (54%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3540/6493 (55%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3550/6493 (55%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3560/6493 (55%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3570/6493 (55%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3580/6493 (55%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3590/6493 (55%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3600/6493 (55%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3610/6493 (56%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3620/6493 (56%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3630/6493 (56%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3640/6493 (56%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3650/6493 (56%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3660/6493 (56%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3670/6493 (57%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3680/6493 (57%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3690/6493 (57%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3700/6493 (57%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3710/6493 (57%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3720/6493 (57%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3730/6493 (57%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3740/6493 (58%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3750/6493 (58%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3760/6493 (58%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3770/6493 (58%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3780/6493 (58%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3790/6493 (58%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3800/6493 (59%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3810/6493 (59%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3820/6493 (59%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3830/6493 (59%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3840/6493 (59%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3850/6493 (59%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3860/6493 (59%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3870/6493 (60%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3880/6493 (60%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3890/6493 (60%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3900/6493 (60%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3910/6493 (60%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3920/6493 (60%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3930/6493 (61%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3940/6493 (61%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3950/6493 (61%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3960/6493 (61%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3970/6493 (61%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3980/6493 (61%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [3990/6493 (61%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4000/6493 (62%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4010/6493 (62%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4020/6493 (62%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4030/6493 (62%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4040/6493 (62%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4050/6493 (62%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4060/6493 (63%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4070/6493 (63%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4080/6493 (63%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4090/6493 (63%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4100/6493 (63%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4110/6493 (63%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4120/6493 (63%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4130/6493 (64%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4140/6493 (64%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4150/6493 (64%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4160/6493 (64%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4170/6493 (64%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4180/6493 (64%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4190/6493 (65%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4200/6493 (65%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4210/6493 (65%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4220/6493 (65%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4230/6493 (65%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4240/6493 (65%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4250/6493 (65%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4260/6493 (66%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4270/6493 (66%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4280/6493 (66%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4290/6493 (66%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4300/6493 (66%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4310/6493 (66%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4320/6493 (67%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4330/6493 (67%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4340/6493 (67%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4350/6493 (67%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4360/6493 (67%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4370/6493 (67%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4380/6493 (67%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4390/6493 (68%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4400/6493 (68%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4410/6493 (68%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4420/6493 (68%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4430/6493 (68%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4440/6493 (68%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4450/6493 (69%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4460/6493 (69%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4470/6493 (69%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4480/6493 (69%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4490/6493 (69%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4500/6493 (69%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4510/6493 (69%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4520/6493 (70%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4530/6493 (70%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4540/6493 (70%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4550/6493 (70%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4560/6493 (70%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4570/6493 (70%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4580/6493 (71%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4590/6493 (71%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4600/6493 (71%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4610/6493 (71%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4620/6493 (71%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4630/6493 (71%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4640/6493 (71%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4650/6493 (72%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4660/6493 (72%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4670/6493 (72%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4680/6493 (72%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4690/6493 (72%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4700/6493 (72%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4710/6493 (73%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4720/6493 (73%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4730/6493 (73%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4740/6493 (73%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4750/6493 (73%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4760/6493 (73%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4770/6493 (73%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4780/6493 (74%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4790/6493 (74%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4800/6493 (74%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4810/6493 (74%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4820/6493 (74%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4830/6493 (74%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4840/6493 (75%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4850/6493 (75%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4860/6493 (75%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4870/6493 (75%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4880/6493 (75%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4890/6493 (75%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4900/6493 (75%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4910/6493 (76%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4920/6493 (76%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4930/6493 (76%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4940/6493 (76%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4950/6493 (76%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4960/6493 (76%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4970/6493 (77%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4980/6493 (77%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [4990/6493 (77%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5000/6493 (77%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5010/6493 (77%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5020/6493 (77%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5030/6493 (77%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5040/6493 (78%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5050/6493 (78%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5060/6493 (78%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5070/6493 (78%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5080/6493 (78%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5090/6493 (78%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5100/6493 (79%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5110/6493 (79%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5120/6493 (79%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5130/6493 (79%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5140/6493 (79%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5150/6493 (79%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5160/6493 (79%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5170/6493 (80%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5180/6493 (80%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5190/6493 (80%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5200/6493 (80%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5210/6493 (80%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5220/6493 (80%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5230/6493 (81%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5240/6493 (81%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5250/6493 (81%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5260/6493 (81%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5270/6493 (81%)]\tLoss: 0.693147\n",
      "Train Epoch: 0 [5280/6493 (81%)]\tLoss: 0.693147\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m start_accuracy \u001b[38;5;241m=\u001b[39m our_model\u001b[38;5;241m.\u001b[39mtest()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting accuracy = \u001b[39m\u001b[38;5;124m\"\u001b[39m, start_accuracy)\n\u001b[1;32m---> 60\u001b[0m \u001b[43mour_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# our_model.paper_train()\u001b[39;00m\n\u001b[0;32m     63\u001b[0m end_accuracy \u001b[38;5;241m=\u001b[39m our_model\u001b[38;5;241m.\u001b[39mtest()\n",
      "Cell \u001b[1;32mIn[2], line 107\u001b[0m, in \u001b[0;36mReweighting.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader):\n\u001b[1;32m--> 107\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_folder = '../dataset-ninja/train_unbiased'\n",
    "# test_folder = '../dataset-ninja/test_unbiased'\n",
    "# validate_folder = '../dataset-ninja/validate_unbiased'\n",
    "train_folder = '../dataset-ninja/train_binary'\n",
    "test_folder = '../dataset-ninja/test_binary'\n",
    "validate_folder = '../dataset-ninja/validate_binary'\n",
    "\n",
    "class_weights = [0.987, 0.013]  # Example weights for each class\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[107.96332894557823, 108.8669668707483, 116.74490789115646], std=[55.43744379373414, 59.496382526025215, 62.06132599239535]),  # Normalize images\n",
    "])\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "# ])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_folder, transform=transform)\n",
    "weights = [class_weights[label] for label in train_dataset.targets]\n",
    "train_sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_folder, transform=transform)\n",
    "\n",
    "validate_dataset = datasets.ImageFolder(validate_folder, transform=transform)\n",
    "\n",
    "\n",
    "# number of epoch and log interval reduced for testing\n",
    "hyperparameters = {\n",
    "    'n_epochs' : 10000,\n",
    "    'batch_size' : 2,\n",
    "    'learning_rate' : 1e-3,\n",
    "    'momentum' : 0.5,\n",
    "    'log_interval' : 5\n",
    "}\n",
    "\n",
    "# network = AlexNet()\n",
    "# network = LeNet()\n",
    "network = JustinNet()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "# criterion_mean = nn.CrossEntropyLoss(reduction='mean')\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "criterion_mean = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD(network.params(),\n",
    "                        lr=hyperparameters['learning_rate'],\n",
    "                        momentum=hyperparameters['momentum'])\n",
    "\n",
    "# Create DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], sampler=train_sampler)\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
    "valid_loader = DataLoader(validate_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "our_model = Reweighting(network, hyperparameters, criterion, criterion_mean, optimizer, train_loader, valid_loader, test_loader)\n",
    "\n",
    "start_accuracy = our_model.test()\n",
    "print(\"Starting accuracy = \", start_accuracy)\n",
    "\n",
    "our_model.train()\n",
    "# our_model.paper_train()\n",
    "\n",
    "end_accuracy = our_model.test()\n",
    "print(\"Ending accuracy = \", end_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_images(dataloader, num_images):\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        for i in range(len(data)):\n",
    "            plt.imshow(data[i][2,:,:].squeeze(), cmap='gray')\n",
    "            plt.title(f\"Label: {target[i]}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            num_images -= 1\n",
    "            if num_images == 0:\n",
    "                return\n",
    "\n",
    "show_images(valid_loader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Ending accuracy =  50.0\n"
     ]
    }
   ],
   "source": [
    "end_accuracy = our_model.test()\n",
    "print(\"Ending accuracy = \", end_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 2, 2, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 7, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:03,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 7,  ..., 2, 2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:05,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([6, 2, 2,  ..., 2, 7, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:06,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 7,  ..., 4, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:07,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:08,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 4,  ..., 2, 7, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:09,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 6, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:10,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 7, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:12,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 2, 2, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:13,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:14,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([4, 2, 2,  ..., 4, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:15,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 2,  ..., 2, 2, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:16,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 7, 7,  ..., 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:18,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 7,  ..., 2, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:19,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 4,  ..., 4, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:20,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([4, 7, 2,  ..., 6, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:21,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 6, 7,  ..., 2, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:22,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 6,  ..., 2, 4, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:24,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 2,  ..., 7, 2, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:25,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 6, 2,  ..., 2, 7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:26,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([7, 2, 2,  ..., 2, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:27,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 2 2 2]\n",
      "tensor([2, 2, 7,  ..., 6, 2, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:27,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "tensor([2, 2, 2, 6, 7, 2, 7, 6, 2, 2, 2, 2, 7, 1, 7, 2, 4, 2, 2, 2, 7, 7, 2, 7,\n",
      "        2, 7, 7, 4, 2, 7, 2, 7, 2, 2, 2, 4, 7, 7, 2, 6, 6, 7, 2, 2, 7, 2, 2, 7,\n",
      "        6, 2, 2, 2, 7, 7, 7, 7, 7, 1, 6, 2, 2, 2, 2, 4, 2, 7, 2, 7, 7, 7, 2, 2,\n",
      "        2, 2, 1, 2, 2, 2, 2, 7, 2, 2, 6, 2, 2, 2, 2, 2, 7, 2, 7, 2, 2, 7, 2, 2,\n",
      "        2, 2, 2, 7, 2, 2, 2, 7, 4, 7, 2, 7, 2, 7, 7, 2, 2, 4, 2, 2, 2, 6, 7, 2,\n",
      "        2, 2, 2, 2, 7, 0, 2, 2, 2, 1, 2, 7, 2, 2, 2, 7, 2, 2, 7, 2, 7, 2, 2, 2,\n",
      "        7, 6, 4, 7, 7, 2, 2, 2, 4, 2, 4, 0, 7, 6, 2, 2, 7, 2, 7, 6, 2, 2, 2, 2,\n",
      "        2, 6, 7, 2, 7, 2, 2, 2, 2, 7, 2, 7, 7, 4, 7, 2, 7, 7, 2, 2, 7, 7, 2, 1,\n",
      "        1, 7, 4, 7, 4, 4, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2,\n",
      "        7, 4, 7, 2, 7, 6, 2, 2, 7, 2, 2, 7, 2, 3, 1, 2, 7, 2, 2, 7, 4, 2, 4, 2,\n",
      "        7, 2, 2, 2, 7, 4, 7, 2, 4, 7, 2, 2, 2, 2, 2, 7, 4, 2, 7, 2, 7, 7, 2, 4,\n",
      "        2, 7, 2, 2, 2, 2, 7, 2, 2, 7, 7, 2, 2, 2, 2, 4, 0, 2, 3, 2, 2, 2, 2, 2,\n",
      "        2, 7, 2, 2, 4, 2, 1, 7, 4, 7, 2, 2, 2, 6, 2, 7, 7, 7, 2, 2])\n",
      "Ending accuracy =  56.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_model.network.eval()\n",
    "\n",
    "acc = []\n",
    "for itr,(test_img, test_label) in tqdm(enumerate(our_model.test_loader)):\n",
    "    prediction = our_model.network(test_img.to(device)).detach().cpu().numpy()\n",
    "    # print(prediction)\n",
    "    prediction = np.argmax(prediction, axis=1)\n",
    "    tmp = (prediction == test_label.detach().numpy())\n",
    "    print(prediction)\n",
    "    print(test_label)\n",
    "    acc.append(tmp)\n",
    "\n",
    "accuracy = np.concatenate(acc).mean()\n",
    "end_accuracy = np.round(accuracy*100,2)\n",
    "\n",
    "print(\"Ending accuracy = \", end_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE50024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
